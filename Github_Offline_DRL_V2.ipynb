{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MK25BM/offline-DRL/blob/main/Github_Offline_DRL_V2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "da5b741f"
      },
      "source": [
        "# Task\n",
        "\n",
        "Create a Gymnasium-compatible wrapper around simglucose (https://github.com/jxx123/simglucose) simulator instance. Generate some offline patient data using the simulator. Wrap the environment with Minari DataCollector.\n",
        "\n",
        "Using the above, demonstrate Offline Deep Reinforcement Learning (DRL) and Off-Policy Evaluation (OPE) by first defining an OpenAI Gym-compatible environment, implementing a behavior policy to collect an offline dataset, then implementing and training an Offline DRL algorithm on this dataset. Subsequently, implement and apply Off-Policy Evaluation (OPE) methods to estimate the performance of the trained offline policy using only the collected data. Finally, visualize the results, and summarize the demonstration, highlighting key findings, challenges of offline RL, and the utility of OPE."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Redo verbose pipeline\n"
      ],
      "metadata": {
        "id": "EEoHhGZKvfVW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Library imports"
      ],
      "metadata": {
        "id": "_sJVuFwmxVOi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import logging\n",
        "from typing import Tuple, Any\n",
        "\n",
        "def setup_dependencies() -> None:\n",
        "    \"\"\"Install and import required packages.\"\"\"\n",
        "    print(\"Setting up dependencies...\")\n",
        "\n",
        "    # Aggressive clean uninstall and cache purge\n",
        "    os.system('pip uninstall -y gym gymnasium numpy minari d3rlpy scipy scikit-learn 2>/dev/null')\n",
        "    os.system('pip cache purge')\n",
        "\n",
        "    # Install compatible versions\n",
        "    os.system('pip install -q \"numpy==1.23.5\" --force-reinstall')\n",
        "    os.system('pip install -q \"scipy==1.9.3\" --force-reinstall')\n",
        "    os.system('pip install -q \"scikit-learn==1.2.2\" --force-reinstall')\n",
        "    os.system('pip install -q d3rlpy minari gymnasium gym --force-reinstall')\n",
        "\n",
        "    print(\"OK All dependencies installed\\n\")\n",
        "\n",
        "def import_libraries() -> Tuple[Any, Any, Any, Any]:\n",
        "    \"\"\"Import required libraries after installation.\"\"\"\n",
        "    import gymnasium\n",
        "    import gym\n",
        "    import d3rlpy\n",
        "    import minari\n",
        "\n",
        "    print(f\"OK All imports successful\")\n",
        "    print(f\"   Minari version: {minari.__version__}\\n\")\n",
        "\n",
        "    return gymnasium, gym, d3rlpy, minari\n",
        "\n",
        "def setup_logging(verbose: bool = True) -> None:\n",
        "    \"\"\"Configure logging for the pipeline.\"\"\"\n",
        "\n",
        "    if verbose:\n",
        "        # Keep INFO level for our custom messages\n",
        "        logging.basicConfig(level=logging.INFO)\n",
        "    else:\n",
        "        # Suppress most logging\n",
        "        logging.basicConfig(level=logging.WARNING)\n",
        "\n",
        "    # Suppress d3rlpy's verbose logging\n",
        "    logging.getLogger('d3rlpy').setLevel(logging.WARNING)\n",
        "    logging.getLogger('minari').setLevel(logging.WARNING)\n",
        "\n",
        "# Call setup_dependencies first to ensure packages are installed\n",
        "setup_dependencies()\n",
        "\n",
        "# Then import the libraries\n",
        "gymnasium, gym, d3rlpy, minari = import_libraries()"
      ],
      "metadata": {
        "id": "Bmj-E0NRphbK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MOCK T1D ENVIRONMENT"
      ],
      "metadata": {
        "id": "x3ctRdZrxgGU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "# Aggressive clean uninstall and cache purge to resolve numpy compatibility issues\n",
        "os.system('pip uninstall -y gym gymnasium numpy minari d3rlpy scipy scikit-learn 2>/dev/null')\n",
        "os.system('pip cache purge')\n",
        "\n",
        "# Install compatible versions forcefully\n",
        "os.system('pip install -q \"numpy==1.23.5\" --force-reinstall')\n",
        "os.system('pip install -q \"scipy==1.9.3\" --force-reinstall')\n",
        "os.system('pip install -q \"scikit-learn==1.2.2\" --force-reinstall')\n",
        "os.system('pip install -q d3rlpy minari gymnasium gym --force-reinstall')\n",
        "\n",
        "import gymnasium\n",
        "import numpy as np\n",
        "from typing import Optional, Tuple, Dict, Any"
      ],
      "metadata": {
        "id": "jREkFdyEzWsN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# MOCK T1D ENVIRONMENT\n",
        "# ============================================================================\n",
        "\n",
        "class MockT1DEnv:\n",
        "    \"\"\"Mock Type 1 Diabetes environment for offline RL training.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"Initialize mock environment.\"\"\"\n",
        "        self.current_glucose = 120.0\n",
        "        self.time_step = 0\n",
        "        self.max_steps = 480\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Reset the environment.\"\"\"\n",
        "        min_glucose = 100.0\n",
        "        max_glucose = 150.0\n",
        "        self.current_glucose = np.random.uniform(min_glucose, max_glucose)\n",
        "        self.time_step = 0\n",
        "        return self.current_glucose\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"Step the environment.\"\"\"\n",
        "        action = float(action)\n",
        "        baseline = 15.0\n",
        "        mean_noise = 0.0\n",
        "        std_noise = 5.0\n",
        "        noise = np.random.normal(mean_noise, std_noise)\n",
        "        factor = 0.5\n",
        "        delta = (action - baseline) * factor + noise\n",
        "        self.current_glucose = self.current_glucose + delta\n",
        "        self.current_glucose = np.clip(self.current_glucose, 40.0, 300.0)\n",
        "\n",
        "        self.time_step = self.time_step + 1\n",
        "        done = self.time_step >= self.max_steps\n",
        "\n",
        "        if self.current_glucose < 70.0:\n",
        "            reward = -1.0\n",
        "        elif self.current_glucose > 180.0:\n",
        "            reward = -0.5\n",
        "        else:\n",
        "            reward = 1.0\n",
        "\n",
        "        info = {'glucose': self.current_glucose}\n",
        "\n",
        "        return self.current_glucose, reward, done, info\n",
        "\n",
        "    def close(self):\n",
        "        \"\"\"Close environment.\"\"\"\n",
        "        pass\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# SIMGLUCOSE ENVIRONMENT WRAPPER - DIRECT INSTANTIATION (Now wraps MockT1DEnv)\n",
        "# ============================================================================\n",
        "\n",
        "class SimglucoseGymEnv(gymnasium.Env):\n",
        "    \"\"\"\n",
        "    Gymnasium-compatible wrapper for SimGlucose T1DSimEnv.\n",
        "    Now directly instantiates MockT1DEnv due to simglucose dependency issues.\n",
        "    \"\"\"\n",
        "\n",
        "    metadata = {\"render_modes\": [\"human\"], \"render_fps\": 30}\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        patient_name: str = 'adolescent#001',\n",
        "        seed: Optional[int] = None,\n",
        "        render_mode: Optional[str] = None\n",
        "    ):\n",
        "        \"\"\"Initialize the SimglucoseGymEnv.\"\"\"\n",
        "        super().__init__() # Removed seed=seed here\n",
        "\n",
        "        self.render_mode = render_mode\n",
        "        self.patient_name = patient_name\n",
        "        self._episode_steps = 0\n",
        "        self._max_episode_steps = 480\n",
        "        self._episode_rewards = []\n",
        "        self._last_obs = None\n",
        "\n",
        "        # Use MockT1DEnv instead of T1DSimEnv due to dependency conflicts\n",
        "        self.env = MockT1DEnv()\n",
        "        print(f\"OK Successfully initialized MockT1DEnv (instead of T1DSimEnv due to compatibility issues).\")\n",
        "\n",
        "        self.action_space = gymnasium.spaces.Box(\n",
        "            low=np.float32(0.0),\n",
        "            high=np.float32(30.0),\n",
        "            shape=(1,),\n",
        "            dtype=np.float32\n",
        "        )\n",
        "\n",
        "        self.observation_space = gymnasium.spaces.Box(\n",
        "            low=np.float32(0.0),\n",
        "            high=np.float32(1000.0),\n",
        "            shape=(1,),\n",
        "            dtype=np.float32\n",
        "        )\n",
        "\n",
        "        # Ensure reproducibility for internal random operations if seed is provided\n",
        "        if seed is not None:\n",
        "            np.random.seed(seed)\n",
        "\n",
        "    def step(self, action: np.ndarray) -> Tuple[np.ndarray, float, bool, bool, dict]:\n",
        "        \"\"\"Perform one step in the environment.\"\"\"\n",
        "        if isinstance(action, np.ndarray):\n",
        "            scalar_action = float(action[0]) if action.size == 1 else float(action)\n",
        "        else:\n",
        "            scalar_action = float(action)\n",
        "\n",
        "        try:\n",
        "            observation, reward, done, info = self.env.step(scalar_action)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"ERROR in step: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            raise\n",
        "\n",
        "        self._episode_steps += 1\n",
        "        self._episode_rewards.append(float(reward))\n",
        "\n",
        "        if observation is None:\n",
        "            observation = self._last_obs if self._last_obs is not None else np.array([0.0], dtype=np.float32)\n",
        "        else:\n",
        "            if not isinstance(observation, np.ndarray):\n",
        "                observation = np.array([float(observation)], dtype=np.float32)\n",
        "            else:\n",
        "                if observation.ndim == 0:\n",
        "                    observation = np.array([float(observation)], dtype=np.float32)\n",
        "                elif observation.shape == (1,):\n",
        "                    observation = observation.astype(np.float32)\n",
        "                else:\n",
        "                    observation = np.array([float(observation.flat[0])], dtype=np.float32)\n",
        "            self._last_obs = observation.copy()\n",
        "\n",
        "        truncated = self._episode_steps >= self._max_episode_steps\n",
        "\n",
        "        return observation, float(reward), bool(done), truncated, info\n",
        "\n",
        "    def reset(\n",
        "        self,\n",
        "        seed: Optional[int] = None,\n",
        "        options: Optional[dict] = None\n",
        "    ) -> Tuple[np.ndarray, dict]:\n",
        "        \"\"\"Reset the environment.\"\"\"\n",
        "        super().reset(seed=seed)\n",
        "\n",
        "        try:\n",
        "            # If a seed is provided to reset, ensure MockT1DEnv uses it for reproducibility\n",
        "            if seed is not None:\n",
        "                np.random.seed(seed)\n",
        "            observation = self.env.reset()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"ERROR in reset: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            raise\n",
        "\n",
        "        if observation is None:\n",
        "            observation = np.array([0.0], dtype=np.float32)\n",
        "        else:\n",
        "            if not isinstance(observation, np.ndarray):\n",
        "                observation = np.array([float(observation)], dtype=np.float32)\n",
        "            else:\n",
        "                if observation.ndim == 0:\n",
        "                    observation = np.array([float(observation)], dtype=np.float32)\n",
        "                elif observation.shape == (1,):\n",
        "                    observation = observation.astype(np.float32)\n",
        "                else:\n",
        "                    observation = np.array([float(observation.flat[0])], dtype=np.float32)\n",
        "\n",
        "        self._last_obs = observation.copy()\n",
        "        self._episode_steps = 0\n",
        "        self._episode_rewards = []\n",
        "\n",
        "        return observation, {}\n",
        "\n",
        "    def render(self) -> Optional[Any]:\n",
        "        \"\"\"Render the environment (if applicable).\"\"\"\n",
        "        return None\n",
        "\n",
        "    def close(self) -> None:\n",
        "        \"\"\"Close the environment and cleanup resources.\"\"\"\n",
        "        if hasattr(self, 'env'):\n",
        "            self.env.close()\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# ENVIRONMENT SETUP AND TESTING\n",
        "# ============================================================================\n",
        "\n",
        "def setup_simglucose_environment(\n",
        "    patient_name: str = 'adolescent#001',\n",
        "    seed: int = 42\n",
        ") -> SimglucoseGymEnv:\n",
        "    \"\"\"Setup and initialize a SimGlucose environment.\"\"\"\n",
        "    print(\"Initializing SimglucoseGymEnv...\")\n",
        "    env = SimglucoseGymEnv(patient_name=patient_name, seed=seed)\n",
        "    print(\"OK SimglucoseGymEnv initialized successfully!\")\n",
        "    return env\n",
        "\n",
        "\n",
        "def test_environment(env: SimglucoseGymEnv, n_steps: int = 5) -> None:\n",
        "    \"\"\"Test the environment with random actions.\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"Testing Environment with Random Actions\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    obs, info = env.reset(seed=42)\n",
        "    print(f\"\\nOK Reset successful!\")\n",
        "    print(f\"Initial Observation: {obs}\")\n",
        "\n",
        "    print(\"\\n--- Testing Steps ---\")\n",
        "    episode_rewards = []\n",
        "\n",
        "    for i in range(n_steps):\n",
        "        action = env.action_space.sample()\n",
        "        obs, reward, terminated, truncated, info = env.step(action)\n",
        "        episode_rewards.append(reward)\n",
        "\n",
        "        print(f\"Step {i + 1}: obs={obs[0]:.2f}, reward={reward:.4f}\")\n",
        "\n",
        "        if terminated or truncated:\n",
        "            print(\"  Episode ended!\")\n",
        "            break\n",
        "\n",
        "    print(f\"\\n--- Episode Summary ---\")\n",
        "    print(f\"  Total Steps: {len(episode_rewards)}\")\n",
        "    print(f\"  Total Return: {sum(episode_rewards):.4f}\")\n",
        "    print(f\"  Average Reward: {np.mean(episode_rewards):.4f}\")\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "\n",
        "\n",
        "def define_behavior_policy(observation: np.ndarray, env: SimglucoseGymEnv) -> np.ndarray:\n",
        "    \"\"\"Simple random behavior policy for data collection.\"\"\"\n",
        "    return env.action_space.sample()"
      ],
      "metadata": {
        "id": "qvUQNZFOrd51"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Collection"
      ],
      "metadata": {
        "id": "8878GPOPxsTJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# DATA COLLECTION\n",
        "# ============================================================================\n",
        "\n",
        "def setup_data_collection(\n",
        "    env: SimglucoseGymEnv,\n",
        "    num_episodes: int = 10,\n",
        "    max_steps_per_episode: int = 480,\n",
        "    dataset_name: str = None  # Add this parameter\n",
        ") -> Tuple[str, DataCollector]: # Modified to return DataCollector\n",
        "    \"\"\"Setup and prepare data collection with Minari DataCollector.\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"Setting up Minari Data Collection\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    # Use provided dataset_name or generate one\n",
        "    if dataset_name is None:\n",
        "        timestamp = time.strftime(\"%Y%m%d%H%M%S\")\n",
        "        dataset_name = f'simglucose/adolescent/random-v{timestamp}'\n",
        "\n",
        "    print(f\"\\nCreating DataCollector for dataset '{dataset_name}'...\")\n",
        "    # Initialize Minari DataCollector without dataset_id in constructor\n",
        "    data_collector = DataCollector(env, record_infos=True)\n",
        "    print(\"OK DataCollector created successfully!\")\n",
        "\n",
        "    print(f\"\\nData Collection Parameters:\")\n",
        "    print(f\"  - Dataset Name: {dataset_name}\")\n",
        "    print(f\"  - Number of Episodes: {num_episodes}\")\n",
        "    print(f\"  - Max Steps per Episode: {max_steps_per_episode}\")\n",
        "\n",
        "    return dataset_name, data_collector\n",
        "\n",
        "\n",
        "def collect_data_simple(\n",
        "    env:   SimglucoseGymEnv,\n",
        "    policy:  Callable,\n",
        "    num_episodes:  int = 10,\n",
        "    max_steps_per_episode: int = 480,\n",
        "    dataset_name: str = 'simglucose-adolescent-random-v0',\n",
        "    verbose: bool = True\n",
        "):\n",
        "    \"\"\"Collect trajectory data and return as dictionary.\"\"\"\n",
        "    if verbose:\n",
        "        print(f\"\\nStarting data collection for {num_episodes} episodes...\")\n",
        "        start_time = time.time()\n",
        "\n",
        "    episodes_data = []\n",
        "\n",
        "    for episode_num in range(num_episodes):\n",
        "        observations = []\n",
        "        actions = []\n",
        "        rewards = []\n",
        "        terminations = []\n",
        "        truncations = []\n",
        "\n",
        "        obs, _ = env.reset()\n",
        "\n",
        "        for step in range(max_steps_per_episode):\n",
        "            observations.append(obs.  copy())\n",
        "            action = policy(obs, env)\n",
        "            actions.append(action)\n",
        "\n",
        "            obs, reward, terminated, truncated, info = env.step(action)\n",
        "            rewards.append(reward)\n",
        "            terminations.append(terminated)\n",
        "            truncations.append(truncated)\n",
        "\n",
        "            if terminated or truncated:\n",
        "                break\n",
        "\n",
        "        episode_dict = {\n",
        "            'observations': np.array(observations),\n",
        "            'actions': np.array(actions),\n",
        "            # ✅ Reshape rewards to 2D:  (n_steps, 1)\n",
        "            'rewards': np.array(rewards, dtype=np.float32).reshape(-1, 1),\n",
        "            'terminations': np.array(terminations),\n",
        "            'truncations': np.  array(truncations),\n",
        "        }\n",
        "        episodes_data.append(episode_dict)\n",
        "\n",
        "    elapsed_time = time.time() - start_time if verbose else None\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"OK Data collection complete! (took {elapsed_time:.2f}s)\")\n",
        "        print(f\"Collected {len(episodes_data)} episodes\")\n",
        "        print(f\"   Total transitions: {sum(len(ep['rewards']) for ep in episodes_data)}\")\n",
        "\n",
        "    return episodes_data\n",
        "\n",
        "\n",
        "def collect_data_and_save(\n",
        "    num_episodes: int = 10,\n",
        "    max_steps_per_episode: int = 480,\n",
        "    patient_name: str = 'adolescent#001',\n",
        "    policy_type: str = 'random'\n",
        "):\n",
        "    \"\"\"Complete SimGlucose environment setup and data collection pipeline.\"\"\"\n",
        "    print(\"=\" * 80)\n",
        "    print(\"SimGlucose Environment Setup and Data Collection\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    env = setup_simglucose_environment(patient_name=patient_name, seed=42)\n",
        "    test_environment(env, n_steps=5)\n",
        "\n",
        "    policy = define_behavior_policy\n",
        "    print(\"\\nOK Using random policy\")\n",
        "\n",
        "    episodes_data = collect_data_simple(\n",
        "        env=env,\n",
        "        policy=policy,\n",
        "        num_episodes=num_episodes,\n",
        "        max_steps_per_episode=max_steps_per_episode,\n",
        "        dataset_name=None,\n",
        "        verbose=True\n",
        "    )\n",
        "\n",
        "    env.close()\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"OK Data Collection Pipeline Complete!\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    return episodes_data"
      ],
      "metadata": {
        "id": "tjS3du7-rhlE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DATASET MANAGEMENT"
      ],
      "metadata": {
        "id": "V2vj1GPqx67O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# MINARI DATASET MANAGEMENT & REPLAY BUFFER MANAGEMENT\n",
        "# ============================================================================\n",
        "\n",
        "class MinariDatasetLoader:\n",
        "    \"\"\"Handler for loading and managing Minari datasets.\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def load_dataset(dataset_name: str = 'simglucose-adolescent-random-v0') -> MinariDataset:\n",
        "        \"\"\"Load a Minari dataset.\"\"\"\n",
        "        print(f\"\\n{'='*80}\")\n",
        "        print(f\"Loading Minari Dataset:  {dataset_name}\")\n",
        "        print(f\"{'='*80}\")\n",
        "\n",
        "        try:\n",
        "            # Try loading the exact dataset_name passed, which might be a pre-existing one\n",
        "            # or a freshly generated unique one from collect_data_and_save.\n",
        "            dataset = load_dataset(dataset_name, download=False)\n",
        "            print(f\"OK Dataset loaded successfully!\")\n",
        "            print(f\"  - Total episodes: {len(dataset.episodes)}\")\n",
        "            total_transitions = sum(ep.transition_count for ep in dataset.episodes)\n",
        "            print(f\"  - Total transitions: {total_transitions}\")\n",
        "            return dataset\n",
        "\n",
        "        except FileNotFoundError:\n",
        "            print(f\"Dataset '{dataset_name}' not found locally. Collecting new data...\")\n",
        "            # If not found, generate a unique name and collect new data\n",
        "            timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
        "            unique_dataset_name = f'simglucose-adolescent-random-v0-{timestamp}'\n",
        "            dataset = collect_data_and_save(\n",
        "                num_episodes=10,\n",
        "                max_steps_per_episode=480,\n",
        "                patient_name='adolescent#001',\n",
        "                policy_type='random'\n",
        "            )\n",
        "            return dataset\n",
        "\n",
        "    @staticmethod\n",
        "    def dataset_statistics(dataset: MinariDataset) -> Dict[str, Any]:\n",
        "        \"\"\"Compute statistics about the dataset.\"\"\"\n",
        "        episodes = dataset.episodes\n",
        "        episode_returns = []\n",
        "        episode_lengths = []\n",
        "        rewards_list = []\n",
        "\n",
        "        for episode in episodes:\n",
        "            episode_reward = 0.0\n",
        "            for i in range(episode.transition_count):\n",
        "                transition = episode[i]\n",
        "                episode_reward += transition.reward\n",
        "                rewards_list.append(transition.reward)\n",
        "            episode_returns.append(episode_reward)\n",
        "            episode_lengths.append(episode.transition_count)\n",
        "\n",
        "        stats = {\n",
        "            'num_episodes': len(episodes),\n",
        "            'total_transitions': sum(episode_lengths),\n",
        "            'mean_episode_return': float(np.mean(episode_returns)),\n",
        "            'std_episode_return': float(np.std(episode_returns)),\n",
        "            'max_episode_return': float(np.max(episode_returns)),\n",
        "            'min_episode_return': float(np.min(episode_returns)),\n",
        "            'mean_episode_length': float(np.mean(episode_lengths)),\n",
        "            'mean_reward': float(np.mean(rewards_list)),\n",
        "            'std_reward': float(np.std(rewards_list)),\n",
        "        }\n",
        "        return stats\n",
        "\n",
        "    @staticmethod\n",
        "    def print_statistics(stats: Dict[str, Any]) -> None:\n",
        "        \"\"\"Print dataset statistics.\"\"\"\n",
        "        print(f\"\\nDataset Statistics:\")\n",
        "        print(f\"  Episodes: {stats['num_episodes']}\")\n",
        "        print(f\"  Total Transitions: {stats['total_transitions']}\")\n",
        "        print(f\"  Mean Episode Return: {stats['mean_episode_return']:.4f} +/- {stats['std_episode_return']:.4f}\")\n",
        "        print(f\"  Mean Episode Length: {stats['mean_episode_length']:.1f}\")\n",
        "        print(f\"  Mean Reward: {stats['mean_reward']:.4f} +/- {stats['std_reward']:.4f}\")\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# REPLAY BUFFER MANAGEMENT\n",
        "# ============================================================================\n",
        "\n",
        "class ReplayBufferManager:\n",
        "    \"\"\"Handler for converting Minari datasets to d3rlpy ReplayBuffer.\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def create_replay_buffer(dataset: MinariDataset) -> Any:\n",
        "        \"\"\"Convert Minari dataset to d3rlpy ReplayBuffer.\"\"\"\n",
        "        print(f\"\\n{'='*80}\")\n",
        "        print(\"Converting Minari Dataset to d3rlpy ReplayBuffer\")\n",
        "        print(f\"{'='*80}\")\n",
        "\n",
        "        try:\n",
        "            replay_buffer = d3rlpy.dataset.create_replay_buffer(\n",
        "                episodes=dataset.episodes\n",
        "            )\n",
        "            print(f\"OK ReplayBuffer created successfully!\")\n",
        "            print(f\"  - Size: {len(replay_buffer)} transitions\")\n",
        "            return replay_buffer\n",
        "        except Exception as e:\n",
        "            print(f\"ERROR creating replay buffer: {e}\")\n",
        "            raise"
      ],
      "metadata": {
        "id": "Jk95HYg8x4ae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OFFLINE RL ALGORITHM CONFIGURATION & TRAINING\n"
      ],
      "metadata": {
        "id": "RHynWrH9yB-2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# OFFLINE RL ALGORITHM CONFIGURATION\n",
        "# ============================================================================\n",
        "\n",
        "class OfflineRLAlgorithm:\n",
        "    \"\"\"Factory for creating offline RL algorithms.\"\"\"\n",
        "\n",
        "    ALGORITHM_CONFIGS = {\n",
        "        'cql': {\n",
        "            'name': 'Conservative Q-Learning (CQL)',\n",
        "            'description': 'Best for general offline RL',\n",
        "            'config_class': d3rlpy.algos.CQLConfig,\n",
        "            'params': {\n",
        "                'actor_learning_rate': 1e-4,\n",
        "                'critic_learning_rate': 3e-4,\n",
        "                'batch_size': 256,\n",
        "                'gamma': 0.99,\n",
        "                'tau': 5e-3,\n",
        "                'alpha_learning_rate': 1e-4,\n",
        "                'conservative_weight': 10.0,\n",
        "                'n_action_samples': 10,\n",
        "            }\n",
        "        },\n",
        "        'iql': {\n",
        "            'name':  'Implicit Q-Learning (IQL)',\n",
        "            'description': 'Avoids querying unseen actions',\n",
        "            'config_class':  d3rlpy.algos.IQLConfig,\n",
        "            'params': {\n",
        "                'actor_learning_rate': 3e-4,\n",
        "                'critic_learning_rate': 3e-4,\n",
        "                'batch_size': 256,\n",
        "                'gamma': 0.99,\n",
        "                'tau': 5e-3,\n",
        "                'expectile':  0.7,\n",
        "                'weight_temp': 3.0,\n",
        "                'max_weight': 100.0,\n",
        "            }\n",
        "        },\n",
        "        'bc': {\n",
        "            'name': 'Behavioral Cloning (BC)',\n",
        "            'description': 'Simple imitation learning baseline',\n",
        "            'config_class':  d3rlpy.algos.BCConfig,\n",
        "            'params': {\n",
        "                'learning_rate': 1e-4,\n",
        "                'batch_size': 256,\n",
        "            }\n",
        "        },\n",
        "    }\n",
        "\n",
        "    @classmethod\n",
        "    def list_algorithms(cls) -> None:\n",
        "        \"\"\"List available algorithms.\"\"\"\n",
        "        print(f\"\\n{'='*80}\")\n",
        "        print(\"Available Offline RL Algorithms\")\n",
        "        print(f\"{'='*80}\")\n",
        "        for algo_type, config in cls.ALGORITHM_CONFIGS.items():\n",
        "            print(f\"\\n  {algo_type.upper()}:\\n    Name: {config['name']}\\n    Description: {config['description']}\")\n",
        "\n",
        "    @classmethod\n",
        "    def create(cls, algo_type: str = 'cql', device: str = 'cpu:0', **custom_params):\n",
        "        \"\"\"Create an offline RL algorithm.\"\"\"\n",
        "        algo_type = algo_type.lower()\n",
        "\n",
        "        if algo_type not in cls.ALGORITHM_CONFIGS:\n",
        "            raise ValueError(f\"Unknown algorithm:  {algo_type}\")\n",
        "\n",
        "        config_info = cls.ALGORITHM_CONFIGS[algo_type]\n",
        "        params = config_info['params'].copy()\n",
        "        params.update(custom_params)\n",
        "\n",
        "        print(f\"\\n{'='*80}\")\n",
        "        print(f\"Creating {config_info['name']}\")\n",
        "        print(f\"{'='*80}\")\n",
        "        print(f\"Device: {device}\")\n",
        "        print(f\"\\nHyperparameters:\")\n",
        "        for key, value in params.items():\n",
        "            print(f\"  - {key}: {value}\")\n",
        "\n",
        "        config = config_info['config_class'](**params)\n",
        "        algo = config.create(device=device)\n",
        "\n",
        "        print(f\"OK {config_info['name']} created successfully!\")\n",
        "        return algo\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# TRAINING\n",
        "# ============================================================================\n",
        "\n",
        "class OfflineRLTrainer:\n",
        "    \"\"\"Trainer for offline RL algorithms.\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def train(\n",
        "        algo,\n",
        "        replay_buffer,\n",
        "        n_steps:  int = 50000,\n",
        "        save_interval: int = 5000,\n",
        "        verbose: bool = True\n",
        "    ) -> Dict[str, Any]:\n",
        "        \"\"\"Train offline RL algorithm.\"\"\"\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"Starting Offline RL Training\")\n",
        "        print(\"=\"*80)\n",
        "        print(\"Training Configuration:\")\n",
        "        print(f\"  - Algorithm: {algo.__class__.__name__}\")\n",
        "        print(f\"  - Total Steps: {n_steps}\")\n",
        "        # ✅ Use . size() instead of len()\n",
        "        print(f\"  - Replay Buffer Size: {replay_buffer.size()}\")\n",
        "        print()\n",
        "\n",
        "        start_time = time.time()\n",
        "\n",
        "        try:\n",
        "            # Train the algorithm\n",
        "            algo.fit(\n",
        "                replay_buffer,\n",
        "                n_steps=n_steps,\n",
        "                show_progress=verbose\n",
        "            )\n",
        "\n",
        "            training_time = time.time() - start_time\n",
        "\n",
        "            if verbose:\n",
        "                print(f\"\\nOK Training complete!\")\n",
        "                print(f\"  - Training Time: {training_time:.2f}s\") # Fixed f-string format\n",
        "                print(f\"  - Steps per Second: {n_steps / training_time:.2f}\")\n",
        "\n",
        "            return {\n",
        "                'n_steps': n_steps,\n",
        "                'training_time': training_time,\n",
        "                'steps_per_second': n_steps / training_time\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            if verbose:\n",
        "                print(f\"ERROR during training: {e}\")\n",
        "            raise\n"
      ],
      "metadata": {
        "id": "sKeZWyvvrhoL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EVALUATION & MODEL PERSISTENCE"
      ],
      "metadata": {
        "id": "44WfTjaeyRxp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ============================================================================\n",
        "# EVALUATION\n",
        "# ============================================================================\n",
        "\n",
        "class PolicyEvaluator:\n",
        "    \"\"\"Handler for evaluating trained policies.\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def evaluate(\n",
        "        algo,\n",
        "        env,\n",
        "        n_episodes: int = 5,\n",
        "        max_steps:  Optional[int] = None,\n",
        "        verbose: bool = True\n",
        "    ) -> Dict[str, float]:\n",
        "        \"\"\"Evaluate a trained policy.\"\"\"\n",
        "        if verbose:\n",
        "            print(f\"\\n{'='*80}\")\n",
        "            print(f\"Evaluating Policy ({n_episodes} episodes)\")\n",
        "            print(f\"{'='*80}\")\n",
        "\n",
        "        episode_returns = []\n",
        "        episode_lengths = []\n",
        "\n",
        "        for episode_num in range(n_episodes):\n",
        "            obs, _ = env.reset()\n",
        "            total_return = 0.0\n",
        "            steps = 0\n",
        "\n",
        "            done = False\n",
        "            while not done:\n",
        "                action = algo.predict(np.expand_dims(obs, axis=0))[0]\n",
        "                obs, reward, terminated, truncated, _ = env.step(action)\n",
        "                total_return += reward\n",
        "                steps += 1\n",
        "\n",
        "                done = terminated or truncated\n",
        "                if max_steps and steps >= max_steps:\n",
        "                    done = True\n",
        "\n",
        "            episode_returns.append(total_return)\n",
        "            episode_lengths.append(steps)\n",
        "\n",
        "            if verbose:\n",
        "                print(f\"  Episode {episode_num + 1:3d}: Return = {total_return:8.2f}, Steps = {steps:3d}\")\n",
        "\n",
        "        mean_return = float(np.mean(episode_returns))\n",
        "        std_return = float(np.std(episode_returns))\n",
        "        mean_length = float(np.mean(episode_lengths))\n",
        "\n",
        "        stats = {\n",
        "            'mean_return': mean_return,\n",
        "            'std_return': std_return,\n",
        "            'max_return': float(np.max(episode_returns)),\n",
        "            'min_return': float(np.min(episode_returns)),\n",
        "            'mean_episode_length': mean_length,\n",
        "        }\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"\\nOK Evaluation Complete!\")\n",
        "            print(f\"  - Mean Return: {mean_return:.4f} +/- {std_return:.4f}\")\n",
        "\n",
        "        return stats\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# MODEL PERSISTENCE\n",
        "# ============================================================================\n",
        "\n",
        "class ModelManager:\n",
        "    \"\"\"Handler for saving and loading models.\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def save_model(algo, save_path: str = \"./offline_rl_model\") -> None:\n",
        "        \"\"\"Save a trained model.\"\"\"\n",
        "        print(f\"\\nSaving model to {save_path}...\")\n",
        "        algo.save_model(save_path)\n",
        "        print(f\"OK Model saved successfully!\")\n",
        "\n",
        "    @staticmethod\n",
        "    def load_model(\n",
        "        algo_type: str,\n",
        "        save_path: str = \"./offline_rl_model\",\n",
        "        device: str = \"cpu:0\"\n",
        "    ):\n",
        "        \"\"\"Load a trained model.\"\"\"\n",
        "        print(f\"\\nLoading model from {save_path}...\")\n",
        "        config_class = OfflineRLAlgorithm.ALGORITHM_CONFIGS[algo_type.lower()]['config_class']\n",
        "        algo = config_class().create(device=device)\n",
        "        algo.load_model(save_path)\n",
        "        print(f\"OK Model loaded successfully!\")\n",
        "        return algo"
      ],
      "metadata": {
        "id": "gLExY9uer_OY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# COMPLETE PIPELINE & MAIN EXECUTION"
      ],
      "metadata": {
        "id": "dTGcdSvIycMw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# COMPLETE PIPELINE\n",
        "# ============================================================================\n",
        "\n",
        "def main(\n",
        "    episodes_data:      List[Dict],  # Accept episodes data directly\n",
        "    algo_type:  str = 'cql',\n",
        "    n_steps:  int = 100000,\n",
        "    n_eval_episodes: int = 5,\n",
        "    device:  str = 'cpu:  0',\n",
        "    save_model_flag:  bool = True\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"Complete offline RL pipeline.\"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"OFFLINE DEEP REINFORCEMENT LEARNING PIPELINE\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    OfflineRLAlgorithm.  list_algorithms()\n",
        "\n",
        "    # Create replay buffer directly from episodes\n",
        "    try:\n",
        "        # ✅ Convert dict episodes to d3rlpy Episode objects\n",
        "        from d3rlpy.dataset import Episode\n",
        "\n",
        "        d3rlpy_episodes = []\n",
        "        for episode_dict in episodes_data:\n",
        "            # ✅ Episode requires (observations, actions, rewards, terminated:   bool)\n",
        "            # terminated should be True if episode ended, False otherwise\n",
        "            episode = Episode(\n",
        "                observations=episode_dict['observations'],\n",
        "                actions=episode_dict['actions'],\n",
        "                rewards=episode_dict['rewards'],\n",
        "                terminated=True  # ✅ Mark as terminated (episode finished)\n",
        "            )\n",
        "            d3rlpy_episodes.append(episode)\n",
        "\n",
        "        # Now create replay buffer with proper Episode objects\n",
        "        replay_buffer = d3rlpy.dataset.create_fifo_replay_buffer(\n",
        "            episodes=d3rlpy_episodes,\n",
        "            limit=1000000  # 1 million transitions max\n",
        "        )\n",
        "        print(f\"\\nOK ReplayBuffer created successfully!\")\n",
        "        # ✅ Use size() method or get total_transitions\n",
        "        buffer_size = replay_buffer.size()\n",
        "        print(f\"  - Size: {buffer_size} transitions\")\n",
        "        results['replay_buffer_size'] = buffer_size\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR creating replay buffer: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        raise\n",
        "\n",
        "    algo = OfflineRLAlgorithm.  create(algo_type=algo_type, device=device)\n",
        "    results['algorithm'] = algo_type\n",
        "\n",
        "    training_stats = OfflineRLTrainer. train(\n",
        "        algo=algo,\n",
        "        replay_buffer=replay_buffer,\n",
        "        n_steps=n_steps,\n",
        "        save_interval=max(1000, n_steps // 10),\n",
        "        verbose=True\n",
        "    )\n",
        "    results['training_stats'] = training_stats\n",
        "\n",
        "    try:\n",
        "        from gymnasium.  wrappers import TimeLimit\n",
        "        eval_env = TimeLimit(\n",
        "            SimglucoseGymEnv(patient_name='adolescent#001', seed=42),\n",
        "            max_episode_steps=480\n",
        "        )\n",
        "\n",
        "        eval_stats = PolicyEvaluator.evaluate(\n",
        "            algo=algo,\n",
        "            env=eval_env,\n",
        "            n_episodes=n_eval_episodes,\n",
        "            max_steps=480,\n",
        "            verbose=True\n",
        "        )\n",
        "        results['evaluation_stats'] = eval_stats\n",
        "        eval_env.close()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nWARNING Error during evaluation: {e}\")\n",
        "\n",
        "    if save_model_flag:\n",
        "        model_path = f\"./offline_rl_{algo_type}_model\"\n",
        "        ModelManager.save_model(algo, save_path=model_path)\n",
        "        results['model_path'] = model_path\n",
        "\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(\"TRAINING SUMMARY\")\n",
        "    print(f\"{'='*80}\")\n",
        "    print(f\"OK Pipeline Complete!\")\n",
        "    print(f\"\\nResults:\")\n",
        "    print(f\"  - Algorithm: {results['algorithm']}\")\n",
        "    print(f\"  - Training Steps: {results['training_stats']['n_steps']}\")\n",
        "    print(f\"  - Training Time: {results['training_stats']['training_time']:.2f}s\")\n",
        "    print(f\"  - Replay Buffer Size: {results['replay_buffer_size']}\")\n",
        "\n",
        "    if 'evaluation_stats' in results:\n",
        "        eval_stats = results['evaluation_stats']\n",
        "        print(f\"  - Evaluation Mean Return: {eval_stats['mean_return']:.4f} +/- {eval_stats['std_return']:.4f}\")\n",
        "\n",
        "    if 'model_path' in results:\n",
        "        print(f\"  - Model Saved:   {results['model_path']}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# MAIN EXECUTION\n",
        "# ============================================================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"=\" * 70)\n",
        "    print(\"PHASE 1: ENVIRONMENT VERIFICATION\")\n",
        "    print(\"=\" * 70)\n",
        "    # ...  Phase 1 code unchanged ...\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"PHASE 2: DATA COLLECTION\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    episodes_data = None\n",
        "    try:\n",
        "        episodes_data = collect_data_and_save(\n",
        "            num_episodes=10,\n",
        "            max_steps_per_episode=480,\n",
        "            patient_name='adolescent#001',\n",
        "            policy_type='random'\n",
        "        )\n",
        "        print(f\"OK Collected {len(episodes_data)} episodes\")\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR during data collection: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"PHASE 3: OFFLINE RL TRAINING\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    try:\n",
        "        if episodes_data is not None:\n",
        "            results = main(\n",
        "                episodes_data=episodes_data,\n",
        "                algo_type='cql',\n",
        "                n_steps=50000,\n",
        "                n_eval_episodes=3,\n",
        "                device='cpu:0',\n",
        "                save_model_flag=True\n",
        "            )\n",
        "            print(\"\\n\" + \"=\" * 80)\n",
        "            print(\"OK TRAINING COMPLETE!\")\n",
        "            print(\"=\" * 80)\n",
        "        else:\n",
        "            print(\"WARNING: No episodes collected.  Skipping training.\")\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR during training: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"OK COMPLETE OFFLINE DRL PIPELINE FINISHED!\")\n",
        "    print(\"=\" * 80)"
      ],
      "metadata": {
        "id": "lCjyP40RsI52"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMUCMuHx72Hhwcr3zXVIeMM",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}