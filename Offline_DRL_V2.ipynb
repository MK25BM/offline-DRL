{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MK25BM/offline-DRL/blob/main/Offline_DRL_V2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "da5b741f"
      },
      "source": [
        "# Task\n",
        "\n",
        "Create a Gymnasium-compatible wrapper around simglucose (https://github.com/jxx123/simglucose) simulator instance. Generate some offline patient data using the simulator. Wrap the environment with Minari DataCollector.\n",
        "\n",
        "Using the above, demonstrate Offline Deep Reinforcement Learning (DRL) and Off-Policy Evaluation (OPE) by first defining an OpenAI Gym-compatible environment, implementing a behavior policy to collect an offline dataset, then implementing and training an Offline DRL algorithm on this dataset. Subsequently, implement and apply Off-Policy Evaluation (OPE) methods to estimate the performance of the trained offline policy using only the collected data. Finally, visualize the results, and summarize the demonstration, highlighting key findings, challenges of offline RL, and the utility of OPE."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Offline Deep Reinforcement Learning Pipeline for SimGlucose T1D Management\n",
        "A complete end-to-end pipeline for data collection, offline RL training, and policy evaluation.\n",
        "\"\"\"\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import sys\n",
        "import os\n",
        "import time\n",
        "import shutil\n",
        "from typing import Optional, Tuple, Dict, List, Any, Callable\n",
        "from dataclasses import dataclass\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "W1R2PIWNpYqe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# CONFIGURATION, DEPENDENCIES & IMPORTS\n",
        "#\n"
      ],
      "metadata": {
        "id": "m6lnBeE4HA8J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Dependency management and library imports for the Offline RL Pipeline.\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import logging\n",
        "from typing import Tuple, Any\n",
        "\n",
        "\n",
        "def setup_dependencies() -> None:\n",
        "    \"\"\"Install and import required packages.\"\"\"\n",
        "    print(\"Setting up dependencies...\")\n",
        "\n",
        "    # Aggressive clean uninstall and cache purge\n",
        "    os.system('pip uninstall -y gym gymnasium numpy minari d3rlpy scipy scikit-learn 2>/dev/null')\n",
        "    os.system('pip cache purge')\n",
        "\n",
        "    # Install compatible versions\n",
        "    os.system('pip install -q \"numpy==1.23.5\" --force-reinstall')\n",
        "    os.system('pip install -q \"scipy==1.9.3\" --force-reinstall')\n",
        "    os.system('pip install -q \"scikit-learn==1.2.2\" --force-reinstall')\n",
        "    os.system('pip install -q d3rlpy minari gymnasium gym --force-reinstall')\n",
        "\n",
        "    print(\"OK All dependencies installed\\n\")\n",
        "\n",
        "\n",
        "def import_libraries() -> Tuple[Any, Any, Any]:\n",
        "    \"\"\"Import required libraries after installation.\"\"\"\n",
        "    global gymnasium, gym, d3rlpy, minari\n",
        "\n",
        "    import gymnasium\n",
        "    import gym\n",
        "    import d3rlpy\n",
        "    import minari\n",
        "\n",
        "    print(f\"OK All imports successful\")\n",
        "    print(f\"   Minari version: {minari.__version__}\\n\")\n",
        "\n",
        "    return gymnasium, gym, d3rlpy\n",
        "\n",
        "\n",
        "def setup_logging(verbose: bool = True) -> None:\n",
        "    \"\"\"Configure logging for the pipeline.\"\"\"\n",
        "\n",
        "    if verbose:\n",
        "        # Keep INFO level for our custom messages\n",
        "        logging.basicConfig(level=logging.INFO)\n",
        "    else:\n",
        "        # Suppress most logging\n",
        "        logging.basicConfig(level=logging.WARNING)\n",
        "\n",
        "    # Suppress d3rlpy's verbose logging\n",
        "    logging.getLogger('d3rlpy').setLevel(logging.WARNING)\n",
        "    logging.getLogger('minari').setLevel(logging.WARNING)"
      ],
      "metadata": {
        "id": "ljaIFvaEs7D-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Dependency management and library imports for the Offline RL Pipeline.\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import logging\n",
        "from typing import Tuple, Any\n",
        "\n",
        "\n",
        "def setup_dependencies() -> None:\n",
        "    \"\"\"Install and import required packages.\"\"\"\n",
        "    print(\"Setting up dependencies...\")\n",
        "\n",
        "    # Aggressive clean uninstall and cache purge\n",
        "    os.system('pip uninstall -y gym gymnasium numpy minari d3rlpy scipy scikit-learn 2>/dev/null')\n",
        "    os.system('pip cache purge')\n",
        "\n",
        "    # Install compatible versions\n",
        "    os.system('pip install -q \"numpy==1.23.5\" --force-reinstall')\n",
        "    os.system('pip install -q \"scipy==1.9.3\" --force-reinstall')\n",
        "    os.system('pip install -q \"scikit-learn==1.2.2\" --force-reinstall')\n",
        "    os.system('pip install -q d3rlpy minari gymnasium gym --force-reinstall')\n",
        "\n",
        "    print(\"OK All dependencies installed\\n\")\n",
        "\n",
        "\n",
        "def import_libraries() -> Tuple[Any, Any, Any]:\n",
        "    \"\"\"Import required libraries after installation.\"\"\"\n",
        "    global gymnasium, gym, d3rlpy, minari\n",
        "\n",
        "    import gymnasium\n",
        "    import gym\n",
        "    import d3rlpy\n",
        "    import minari\n",
        "\n",
        "    print(f\"OK All imports successful\")\n",
        "    print(f\"   Minari version: {minari.__version__}\\n\")\n",
        "\n",
        "    return gymnasium, gym, d3rlpy\n",
        "\n",
        "\n",
        "def setup_logging(verbose: bool = True) -> None:\n",
        "    \"\"\"Configure logging for the pipeline.\"\"\"\n",
        "\n",
        "    if verbose:\n",
        "        # Keep INFO level for our custom messages\n",
        "        logging.basicConfig(level=logging.INFO)\n",
        "    else:\n",
        "        # Suppress most logging\n",
        "        logging.basicConfig(level=logging.WARNING)\n",
        "\n",
        "    # Suppress d3rlpy's verbose logging\n",
        "    logging.getLogger('d3rlpy').setLevel(logging.WARNING)\n",
        "    logging.getLogger('minari').setLevel(logging.WARNING)"
      ],
      "metadata": {
        "id": "Di73JYZsbvoP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ENVIRONMENTS"
      ],
      "metadata": {
        "id": "mgfduu8fHUBe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# ENVIRONMENTS\n",
        "# ============================================================================\n",
        "\n",
        "\"\"\"\n",
        "Environment definitions for Type 1 Diabetes simulation.\n",
        "\"\"\"\n",
        "\n",
        "from typing import Optional, Tuple, Dict, Any\n",
        "import numpy as np\n",
        "import gymnasium\n",
        "\n",
        "\n",
        "class MockT1DEnv:\n",
        "    \"\"\"Mock Type 1 Diabetes environment.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.current_glucose = 120.0\n",
        "        self.time_step = 0\n",
        "        self.max_steps = 480\n",
        "\n",
        "    def reset(self) -> float:\n",
        "        self.current_glucose = np.random.uniform(100.0, 150.0)\n",
        "        self.time_step = 0\n",
        "        return self.current_glucose\n",
        "\n",
        "    def step(self, action: float) -> Tuple[float, float, bool, Dict]:\n",
        "        action = float(action)\n",
        "        delta = (action - 15.0) * 0.5 + np.random.normal(0.0, 5.0)\n",
        "        self.current_glucose = np.clip(self.current_glucose + delta, 40.0, 300.0)\n",
        "        self.time_step += 1\n",
        "\n",
        "        if self.current_glucose < 70.0:\n",
        "            reward = -1.0\n",
        "        elif self.current_glucose > 180.0:\n",
        "            reward = -0.5\n",
        "        else:\n",
        "            reward = 1.0\n",
        "\n",
        "        done = self.time_step >= self.max_steps\n",
        "        return self.current_glucose, reward, done, {'glucose': self.current_glucose}\n",
        "\n",
        "    def close(self) -> None:\n",
        "        pass\n",
        "\n",
        "\n",
        "class SimglucoseGymEnv(gymnasium.Env):\n",
        "    \"\"\"Gymnasium-compatible SimGlucose wrapper.\"\"\"\n",
        "\n",
        "    metadata = {\"render_modes\": [\"human\"], \"render_fps\": 30}\n",
        "\n",
        "    def __init__(self, patient_name: str = 'adolescent#001', seed: Optional[int] = None, verbose: bool = True):\n",
        "        super().__init__()\n",
        "        self.patient_name = patient_name\n",
        "        self._episode_steps = 0\n",
        "        self._max_episode_steps = 480\n",
        "        self. env = MockT1DEnv()\n",
        "        self.verbose = verbose\n",
        "\n",
        "        self.action_space = gymnasium.spaces.Box(0.0, 30.0, (1,), dtype=np.float32)\n",
        "        self.observation_space = gymnasium.spaces.Box(0.0, 1000.0, (1,), dtype=np.float32)\n",
        "\n",
        "        if seed is not None:\n",
        "            np.random.seed(seed)\n",
        "\n",
        "        if self.verbose:\n",
        "            print(f\"OK Successfully initialized MockT1DEnv\")\n",
        "            print(f\"OK SimglucoseGymEnv initialized successfully!\")\n",
        "\n",
        "    def step(self, action: np.ndarray) -> Tuple[np.ndarray, float, bool, bool, Dict]:\n",
        "        scalar_action = float(action[0]) if isinstance(action, np.ndarray) else float(action)\n",
        "        obs, reward, done, info = self.env.step(scalar_action)\n",
        "\n",
        "        obs = np.array([obs], dtype=np.float32)\n",
        "        self._episode_steps += 1\n",
        "        truncated = self._episode_steps >= self._max_episode_steps\n",
        "\n",
        "        return obs, float(reward), bool(done), truncated, info\n",
        "\n",
        "    def reset(self, seed: Optional[int] = None, options: Optional[Dict] = None) -> Tuple[np.ndarray, Dict]:\n",
        "        super().reset(seed=seed)\n",
        "        if seed is not None:\n",
        "            np.random.seed(seed)\n",
        "\n",
        "        obs = np.array([self.env.reset()], dtype=np.float32)\n",
        "        self._episode_steps = 0\n",
        "        return obs, {}\n",
        "\n",
        "    def render(self) -> None:\n",
        "        pass\n",
        "\n",
        "    def close(self) -> None:\n",
        "        self.env. close()"
      ],
      "metadata": {
        "id": "sU5fIhhypq47"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DATA COLLECTION"
      ],
      "metadata": {
        "id": "w9LuB0nWHcl6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# DATA COLLECTION\n",
        "# ============================================================================\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Data collection module for the Offline RL Pipeline.\n",
        "\"\"\"\n",
        "\n",
        "from typing import List, Dict, Callable, Any\n",
        "import time\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class DataCollector:\n",
        "    \"\"\"Collect trajectory data from environment.\"\"\"\n",
        "\n",
        "    def __init__(self, env, verbose: bool = True):\n",
        "        self.env = env\n",
        "        self.verbose = verbose\n",
        "\n",
        "    def collect(\n",
        "        self,\n",
        "        policy:  Callable,\n",
        "        num_episodes: int = 10,\n",
        "        max_steps:  int = 480\n",
        "    ) -> List[Dict[str, np.ndarray]]:\n",
        "        \"\"\"Collect episodes using a policy.\"\"\"\n",
        "        if self.verbose:\n",
        "            print(\"\\n\" + \"=\" * 80)\n",
        "            print(\"Testing Environment with Random Actions\")\n",
        "            print(\"=\" * 80)\n",
        "\n",
        "            obs, _ = self.env.reset()\n",
        "            print(f\"\\nOK Reset successful!\")\n",
        "            print(f\"Initial Observation: {obs}\")\n",
        "            print(\"\\n--- Testing Steps ---\")\n",
        "\n",
        "            for i in range(5):\n",
        "                action = policy(obs, self.env)\n",
        "                obs, reward, _, _, _ = self.env.step(action)\n",
        "                print(f\"Step {i + 1}: obs={obs[0]:. 2f}, reward={reward:.4f}\")\n",
        "\n",
        "            print(\"\\n\" + \"=\" * 80)\n",
        "            print(f\"\\nStarting data collection for {num_episodes} episodes...\")\n",
        "\n",
        "        start_time = time.time()\n",
        "        episodes = []\n",
        "\n",
        "        for _ in range(num_episodes):\n",
        "            obs, _ = self.env.reset()\n",
        "            episode_data = {'observations': [], 'actions': [], 'rewards': []}\n",
        "\n",
        "            for _ in range(max_steps):\n",
        "                episode_data['observations'].append(obs. copy())\n",
        "                action = policy(obs, self.env)\n",
        "                episode_data['actions'].append(action)\n",
        "\n",
        "                obs, reward, done, truncated, _ = self.env.step(action)\n",
        "                episode_data['rewards'].append(reward)\n",
        "\n",
        "                if done or truncated:\n",
        "                    break\n",
        "\n",
        "            episodes.append({\n",
        "                'observations': np.array(episode_data['observations']),\n",
        "                'actions': np.array(episode_data['actions']),\n",
        "                'rewards': np.array(episode_data['rewards'], dtype=np.float32).reshape(-1, 1),\n",
        "            })\n",
        "\n",
        "        elapsed = time.time() - start_time\n",
        "        total_transitions = sum(len(ep['rewards']) for ep in episodes)\n",
        "\n",
        "        if self.verbose:\n",
        "            print(f\"OK Data collection complete! (took {elapsed:.2f}s)\")\n",
        "            print(f\"Collected {len(episodes)} episodes\")\n",
        "            print(f\"   Total transitions: {total_transitions}\")\n",
        "\n",
        "        return episodes\n",
        "\n",
        "\n",
        "def random_policy(obs: np.ndarray, env) -> np.ndarray:\n",
        "    \"\"\"Random exploration policy.\"\"\"\n",
        "    return env.action_space.sample()"
      ],
      "metadata": {
        "id": "TZIwBvpbpwot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TRAINING & EVALUATION"
      ],
      "metadata": {
        "id": "F3AtZsWMHf6n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# TRAINING & EVALUATION\n",
        "# ============================================================================\n",
        "\n",
        "\"\"\"\n",
        "Training and evaluation module for the Offline RL Pipeline.\n",
        "\"\"\"\n",
        "\n",
        "from typing import List, Dict, Any\n",
        "import time\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class OfflineRLPipeline:\n",
        "    \"\"\"Complete offline RL training pipeline.\"\"\"\n",
        "\n",
        "    ALGORITHMS = {\n",
        "        'cql':  {\n",
        "            'name': 'Conservative Q-Learning',\n",
        "            'config': None,  # Will be set after d3rlpy import\n",
        "            'params': {\n",
        "                'actor_learning_rate': 1e-4,\n",
        "                'critic_learning_rate': 3e-4,\n",
        "                'batch_size': 256,\n",
        "                'gamma': 0.99,\n",
        "                'tau': 5e-3,\n",
        "                'conservative_weight': 10.0,\n",
        "            }\n",
        "        },\n",
        "    }\n",
        "\n",
        "    def __init__(self, config, d3rlpy_module, setup_logging_func):\n",
        "        self.config = config\n",
        "        self.d3rlpy = d3rlpy_module\n",
        "        self.env = None\n",
        "        self.algo = None\n",
        "        self.results = {}\n",
        "\n",
        "        # Set the config class after d3rlpy import\n",
        "        self. ALGORITHMS['cql']['config'] = self. d3rlpy.algos.CQLConfig\n",
        "\n",
        "        # Setup logging based on verbose flag\n",
        "        setup_logging_func(verbose=config.verbose)\n",
        "\n",
        "    def create_environment(self, SimglucoseGymEnv):\n",
        "        \"\"\"Initialize environment.\"\"\"\n",
        "        self.env = SimglucoseGymEnv(\n",
        "            patient_name=self.config.patient_name,\n",
        "            seed=self.config.seed,\n",
        "            verbose=self.config.verbose\n",
        "        )\n",
        "\n",
        "    def collect_data(self, collector) -> List[Dict]:\n",
        "        \"\"\"Collect training data.\"\"\"\n",
        "        if self.config.verbose:\n",
        "            print(\"\\n\" + \"=\" * 80)\n",
        "            print(\"PHASE 1: DATA COLLECTION\")\n",
        "            print(\"=\" * 80)\n",
        "            print(\"SimGlucose Environment Setup and Data Collection\")\n",
        "            print(\"=\" * 80)\n",
        "\n",
        "        episodes = collector.collect(\n",
        "            policy=self._get_behavior_policy(), # Changed this line\n",
        "            num_episodes=self. config.num_episodes,\n",
        "            max_steps=self.config.max_steps_per_episode\n",
        "        )\n",
        "\n",
        "        if self.config.verbose:\n",
        "            print(\"\\n\" + \"=\" * 80)\n",
        "            print(\"OK Data Collection Pipeline Complete!\")\n",
        "            print(\"=\" * 80)\n",
        "            print(f\"OK Collected {len(episodes)} episodes\\n\")\n",
        "\n",
        "        self.env. close()\n",
        "        return episodes\n",
        "\n",
        "    def _get_behavior_policy(self):\n",
        "        \"\"\"Get the behavior policy (e.g., epsilon-greedy or purely random).\"\"\"\n",
        "        epsilon = self.config.epsilon\n",
        "\n",
        "        def epsilon_greedy_policy(obs: np.ndarray, env) -> np.ndarray:\n",
        "            if np.random.rand() < epsilon:\n",
        "                # Explore: take a random action\n",
        "                return env.action_space.sample()\n",
        "            else:\n",
        "                # Exploit: take a greedy action based on heuristic\n",
        "                glucose = obs[0]  # Assuming obs is [glucose_value]\n",
        "\n",
        "                # Simple heuristic for greedy action in glucose regulation\n",
        "                # Action space is a single float between 0.0 and 30.0 (insulin dosage)\n",
        "                if glucose > 180.0:  # High glucose, need more insulin\n",
        "                    greedy_action = np.array([25.0], dtype=np.float32)\n",
        "                elif glucose < 70.0: # Low glucose, need less insulin\n",
        "                    greedy_action = np.array([5.0], dtype=np.float32)\n",
        "                else:  # In target range, maintain baseline\n",
        "                    greedy_action = np.array([15.0], dtype=np.float32)\n",
        "\n",
        "                # Ensure action is within bounds\n",
        "                greedy_action = np.clip(greedy_action, env.action_space.low, env.action_space.high)\n",
        "                return greedy_action\n",
        "\n",
        "        if epsilon is not None and epsilon > 0:\n",
        "            if self.config.verbose:\n",
        "                print(f\"Using Epsilon-Greedy Behavior Policy with epsilon={epsilon}\")\n",
        "            return epsilon_greedy_policy\n",
        "        else:\n",
        "            if self.config.verbose:\n",
        "                print(\"Using Purely Random Behavior Policy (epsilon=0 or not set)\")\n",
        "            def random_policy(obs: np.ndarray, env) -> np.ndarray:\n",
        "                return env.action_space.sample()\n",
        "            return random_policy\n",
        "\n",
        "    def create_replay_buffer(self, episodes: List[Dict]) -> Any:\n",
        "        \"\"\"Convert episodes to d3rlpy replay buffer.\"\"\"\n",
        "        if self.config.verbose:\n",
        "            print(\"Creating replay buffer...\")\n",
        "\n",
        "        Episode = self.d3rlpy.dataset.Episode\n",
        "\n",
        "        d3rlpy_episodes = [\n",
        "            Episode(\n",
        "                observations=ep['observations'],\n",
        "                actions=ep['actions'],\n",
        "                rewards=ep['rewards'],\n",
        "                terminated=True\n",
        "            )\n",
        "            for ep in episodes\n",
        "        ]\n",
        "\n",
        "        buffer = self.d3rlpy.dataset.create_fifo_replay_buffer(\n",
        "            episodes=d3rlpy_episodes,\n",
        "            limit=1000000\n",
        "        )\n",
        "\n",
        "        if self.config.verbose:\n",
        "            print(f\"OK ReplayBuffer created successfully!\")\n",
        "            print(f\"  - Size: {buffer.size()} transitions\\n\")\n",
        "\n",
        "        return buffer\n",
        "\n",
        "    def create_algorithm(self) -> None:\n",
        "        \"\"\"Create offline RL algorithm.\"\"\"\n",
        "        if self.config.verbose:\n",
        "            print(\"=\" * 80)\n",
        "            print(f\"PHASE 2: Creating {self.config.algo_type. upper()} Algorithm\")\n",
        "            print(\"=\" * 80)\n",
        "\n",
        "        algo_config = self.ALGORITHMS. get(self.config.algo_type)\n",
        "        if not algo_config:\n",
        "            raise ValueError(f\"Unknown algorithm:  {self.config.algo_type}\")\n",
        "\n",
        "        config_class = algo_config['config']\n",
        "\n",
        "        if self.config.verbose:\n",
        "            print(f\"\\n================================================================================\")\n",
        "            print(f\"Creating {algo_config['name']} (CQL)\")\n",
        "            print(f\"================================================================================\")\n",
        "            print(f\"Device: {self.config.device}\")\n",
        "            print(f\"\\nHyperparameters:\")\n",
        "            for key, value in algo_config['params'].items():\n",
        "                print(f\"  - {key}: {value}\")\n",
        "\n",
        "        self.algo = config_class(**algo_config['params']).create(device=self.config.device)\n",
        "\n",
        "        if self.config. verbose:\n",
        "            print(f\"OK {algo_config['name']} created successfully!\\n\")\n",
        "\n",
        "    def train(self, replay_buffer: Any) -> None:\n",
        "        \"\"\"Train the algorithm.\"\"\"\n",
        "        if self.config.verbose:\n",
        "            print(\"=\" * 80)\n",
        "            print(\"PHASE 3: OFFLINE RL TRAINING\")\n",
        "            print(\"=\" * 80)\n",
        "            print(\"=\" * 80)\n",
        "            print(\"OFFLINE DEEP REINFORCEMENT LEARNING PIPELINE\")\n",
        "            print(\"=\" * 80)\n",
        "            print(\"=\" * 80)\n",
        "            print(\"Starting Offline RL Training\")\n",
        "            print(\"=\" * 80)\n",
        "            print(\"Training Configuration:\")\n",
        "            print(f\"  - Algorithm: {self.config.algo_type. upper()}\")\n",
        "            print(f\"  - Total Steps: {self.config.training_steps}\")\n",
        "            print(f\"  - Replay Buffer Size: {replay_buffer.size()}\\n\")\n",
        "\n",
        "        start_time = time.time()\n",
        "        self.algo.fit(\n",
        "            replay_buffer,\n",
        "            n_steps=self.config.training_steps,\n",
        "            show_progress=True\n",
        "        )\n",
        "\n",
        "        elapsed = time.time() - start_time\n",
        "        self.results['training_time'] = elapsed\n",
        "        self.results['steps_per_sec'] = self.config.training_steps / elapsed\n",
        "\n",
        "        if self.config.verbose:\n",
        "            print(f\"\\nOK Training complete in {elapsed:.2f}s\")\n",
        "\n",
        "    def evaluate(self, SimglucoseGymEnv) -> None:\n",
        "        \"\"\"Evaluate trained policy.\"\"\"\n",
        "        if self.config. verbose:\n",
        "            print(\"\\n\" + \"=\" * 80)\n",
        "            print(\"PHASE 4: EVALUATION\")\n",
        "            print(\"=\" * 80)\n",
        "\n",
        "        env = SimglucoseGymEnv(seed=self.config.seed, verbose=False)\n",
        "        returns = []\n",
        "\n",
        "        for i in range(self.config.eval_episodes):\n",
        "            obs, _ = env.reset()\n",
        "            total_return = 0.0\n",
        "            steps = 0\n",
        "\n",
        "            done = False\n",
        "            while not done and steps < self.config.max_steps_per_episode:\n",
        "                action = self.algo.predict(np.expand_dims(obs, axis=0))[0]\n",
        "                obs, reward, terminated, truncated, _ = env.step(action)\n",
        "                total_return += reward\n",
        "                steps += 1\n",
        "                done = terminated or truncated\n",
        "\n",
        "            returns.append(total_return)\n",
        "\n",
        "            if self.config.verbose:\n",
        "                print(f\"  Episode {i+1}:  Return = {total_return:.2f}, Steps = {steps}\")\n",
        "\n",
        "        env.close()\n",
        "\n",
        "        mean_return = np.mean(returns)\n",
        "        std_return = np.std(returns)\n",
        "\n",
        "        self.results['eval_mean_return'] = float(mean_return)\n",
        "        self.results['eval_std_return'] = float(std_return)\n",
        "\n",
        "        if self.config.verbose:\n",
        "            print(f\"\\nOK Mean return: {mean_return:.4f} \\u00B1 {std_return:.4f}\")\n",
        "\n",
        "    def save_model(self) -> None:\n",
        "        \"\"\"Save trained model.\"\"\"\n",
        "        if self.config.save_model and self.algo is not None:\n",
        "            model_path = f\"./offline_rl_{self.config.algo_type}_model\"\n",
        "            self.algo.save_model(model_path)\n",
        "\n",
        "            if self.config.verbose:\n",
        "                print(f\"\\nOK Model saved to {model_path}\")\n",
        "\n",
        "            self.results['model_path'] = model_path\n",
        "\n",
        "    def run(self, collector, SimglucoseGymEnv) -> Dict[str, Any]:\n",
        "        \"\"\"Execute complete pipeline.\"\"\"\n",
        "        try:\n",
        "            self.create_environment(SimglucoseGymEnv)\n",
        "            episodes = self.collect_data(collector)\n",
        "            replay_buffer = self.create_replay_buffer(episodes)\n",
        "            self.create_algorithm()\n",
        "            self.train(replay_buffer)\n",
        "            self.evaluate(SimglucoseGymEnv)\n",
        "            self.save_model()\n",
        "\n",
        "            if self.config.verbose:\n",
        "                self.print_summary()\n",
        "\n",
        "            return self. results\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\nERROR:  {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            raise\n",
        "\n",
        "    def print_summary(self) -> None:\n",
        "        \"\"\"Print training summary.\"\"\"\n",
        "        print(\"\\n\" + \"=\" * 80)\n",
        "        print(\"SUMMARY\")\n",
        "        print(\"=\" * 80)\n",
        "        print(f\"Algorithm: {self. config.algo_type.upper()}\")\n",
        "        print(f\"Training Steps: {self.config.training_steps}\")\n",
        "        print(f\"Training Time: {self.results['training_time']:.2f}s\")\n",
        "        print(f\"Speed: {self.results['steps_per_sec']:.2f} steps/sec\")\n",
        "        print(f\"Eval Return: {self.results['eval_mean_return']:.4f} \\u00B1 {self.results['eval_std_return']:.4f}\")\n",
        "        if 'model_path' in self. results:\n",
        "            print(f\"Model:  {self.results['model_path']}\")\n",
        "        print(\"=\" * 80)"
      ],
      "metadata": {
        "id": "1KVgFnfwp3Yk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MAIN"
      ],
      "metadata": {
        "id": "_IcolY70Ie9Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# ============================================================================\n",
        "# CONFIGURATION\n",
        "# ============================================================================\n",
        "from dataclasses import dataclass\n",
        "from typing import Optional\n",
        "\n",
        "@dataclass\n",
        "class PipelineConfig:\n",
        "    num_episodes: int = 10\n",
        "    training_steps: int = 50000\n",
        "    eval_episodes: int = 3\n",
        "    algo_type: str = 'cql'\n",
        "    verbose: bool = True  # Added missing verbose parameter\n",
        "    patient_name: str = 'adolescent#001'\n",
        "    seed: Optional[int] = None\n",
        "    max_steps_per_episode: int = 480\n",
        "    device: str = 'cpu'\n",
        "    save_model: bool = False\n",
        "    epsilon: float = 0.20  # Changed default to 0.20 to force re-evaluation\n",
        "\n",
        "# ============================================================================\n",
        "# CELL 2: Dependencies\n",
        "# ============================================================================\n",
        "# Run Cell_2_dependencies.py first to import all dependencies\n",
        "\n",
        "# ============================================================================\n",
        "# CELL 3: Environments\n",
        "# ============================================================================\n",
        "# Run Cell_3_environments.py first to define MockT1DEnv and SimglucoseGymEnv\n",
        "\n",
        "# ============================================================================\n",
        "# CELL 4: Data Collection\n",
        "# ============================================================================\n",
        "# Run Cell_4_data_collection.py first to define DataCollector\n",
        "\n",
        "# ============================================================================\n",
        "# CELL 5: Training\n",
        "# ============================================================================\n",
        "# Run Cell_5_training.py first to define OfflineRLPipeline\n",
        "\n",
        "# ============================================================================\n",
        "# CELL 6: Main Execution\n",
        "# ============================================================================\n",
        "\n",
        "def main():\n",
        "    \"\"\"Run the complete offline RL pipeline.\"\"\"\n",
        "\n",
        "    # Setup dependencies and imports\n",
        "    setup_dependencies()\n",
        "    gymnasium, gym, d3rlpy, minari_lib = import_libraries()\n",
        "\n",
        "    # Configure pipeline\n",
        "    config = PipelineConfig(\n",
        "        num_episodes=10,\n",
        "        training_steps=50000,\n",
        "        eval_episodes=3,\n",
        "        algo_type='cql'\n",
        "        # verbose=True,  # Rely on default value from dataclass definition\n",
        "        # epsilon=0.2    # Rely on default value from dataclass definition\n",
        "    )\n",
        "\n",
        "    # Setup logging\n",
        "    setup_logging(verbose=config.verbose)\n",
        "\n",
        "    # Initialize pipeline\n",
        "    pipeline = OfflineRLPipeline(config, d3rlpy, setup_logging)\n",
        "\n",
        "    # Create data collector\n",
        "    collector = DataCollector(None, verbose=config.verbose)\n",
        "\n",
        "    # Run the pipeline\n",
        "    results = pipeline.run(collector, SimglucoseGymEnv)\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "# Execute the main pipeline\n",
        "if __name__ == \"__main__\":\n",
        "    results = main()\n",
        "    print(\"\\n✅ Pipeline execution completed successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "8VAamLFD8sIK",
        "outputId": "7d53520e-6697-4437-c804-4f81d6aa8068"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting up dependencies...\n",
            "OK All dependencies installed\n",
            "\n",
            "OK All imports successful\n",
            "   Minari version: 0.5.3\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "PipelineConfig.__init__() got an unexpected keyword argument 'verbose'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2627639357.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;31m# Execute the main pipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n✅ Pipeline execution completed successfully!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2627639357.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;31m# Configure pipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     config = PipelineConfig(\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0mnum_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mtraining_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: PipelineConfig.__init__() got an unexpected keyword argument 'verbose'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Redo verbose pipeline\n"
      ],
      "metadata": {
        "id": "EEoHhGZKvfVW"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "anqFQIYpvnCu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Library imports"
      ],
      "metadata": {
        "id": "_sJVuFwmxVOi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import logging\n",
        "from typing import Tuple, Any\n",
        "import gymnasium\n",
        "import gym\n",
        "import d3rlpy\n",
        "import minari\n",
        "\n",
        "def setup_dependencies() -> None:\n",
        "    \"\"\"Install and import required packages.\"\"\"\n",
        "    print(\"Setting up dependencies...\")\n",
        "\n",
        "    # Aggressive clean uninstall and cache purge\n",
        "    os.system('pip uninstall -y gym gymnasium numpy minari d3rlpy scipy scikit-learn 2>/dev/null')\n",
        "    os.system('pip cache purge')\n",
        "\n",
        "    # Install compatible versions\n",
        "    os.system('pip install -q \"numpy==1.23.5\" --force-reinstall')\n",
        "    os.system('pip install -q \"scipy==1.9.3\" --force-reinstall')\n",
        "    os.system('pip install -q \"scikit-learn==1.2.2\" --force-reinstall')\n",
        "    os.system('pip install -q d3rlpy minari gymnasium gym --force-reinstall')\n",
        "\n",
        "    print(\"OK All dependencies installed\\n\")\n",
        "\n",
        "def import_libraries() -> Tuple[Any, Any, Any, Any]:\n",
        "    \"\"\"Import required libraries after installation.\"\"\"\n",
        "    import gymnasium\n",
        "    import gym\n",
        "    import d3rlpy\n",
        "    import minari\n",
        "\n",
        "    print(f\"OK All imports successful\")\n",
        "    print(f\"   Minari version: {minari.__version__}\\n\")\n",
        "\n",
        "    return gymnasium, gym, d3rlpy, minari\n",
        "\n",
        "def setup_logging(verbose: bool = True) -> None:\n",
        "    \"\"\"Configure logging for the pipeline.\"\"\"\n",
        "\n",
        "    if verbose:\n",
        "        # Keep INFO level for our custom messages\n",
        "        logging.basicConfig(level=logging.INFO)\n",
        "    else:\n",
        "        # Suppress most logging\n",
        "        logging.basicConfig(level=logging.WARNING)\n",
        "\n",
        "    # Suppress d3rlpy's verbose logging\n",
        "    logging.getLogger('d3rlpy').setLevel(logging.WARNING)\n",
        "    logging.getLogger('minari').setLevel(logging.WARNING)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 498
        },
        "id": "Bmj-E0NRphbK",
        "outputId": "277e98ec-8c12-482c-a75f-ccc3a15c507c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
            "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
            "Users of this version of Gym should be able to simply replace 'import gym' with 'import gymnasium as gym' in the vast majority of cases.\n",
            "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "module 'numpy._core._multiarray_umath' has no attribute '_blas_supports_fpe'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3817834851.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgymnasium\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0md3rlpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mminari\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/d3rlpy/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m from . import (\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0malgos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/d3rlpy/algos/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mqlearning\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtransformer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutility\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/d3rlpy/algos/qlearning/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mawac\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbcq\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbear\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/d3rlpy/algos/qlearning/awac.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0mtypes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mShape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mQLearningAlgoBase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawac_impl\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAWACImpl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msac_impl\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSACModules\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/d3rlpy/algos/qlearning/torch/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0miql_impl\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mplas_impl\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mprdc_impl\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mrebrac_impl\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0msac_impl\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/d3rlpy/algos/qlearning/torch/prdc_impl.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneighbors\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNearestNeighbors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mActionOutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mContinuousEnsembleQFunctionForwarder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;31m# it and importing it first would fail if the OpenMP dll cannot be found.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__check_build\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_distributor_init\u001b[0m  \u001b[0;31m# noqa: E402 F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclone\u001b[0m  \u001b[0;31m# noqa: E402\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_show_versions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mshow_versions\u001b[0m  \u001b[0;31m# noqa: E402\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInconsistentVersionWarning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_metadata_requests\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_MetadataRequester\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_routing_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_missing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mis_pandas_na\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_scalar_nan\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_param_validation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mvalidate_parameter_constraints\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmetadata_routing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bunch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBunch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_chunking\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgen_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen_even_slices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Make _safe_indexing importable from here for backward compat as this particular\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/_chunking.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_param_validation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInterval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcsr_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0missparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/scipy/sparse/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimportlib\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_importlib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_base\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_csr\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_csc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/scipy/sparse/_base.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0moperator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m from ._sputils import (asmatrix, check_reshape_kwargs, check_shape,\n\u001b[0m\u001b[1;32m      9\u001b[0m                        \u001b[0mget_sum_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0misdense\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0misscalarlike\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_todata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                        matrix, validateaxis, getdtype, is_pydata_spmatrix)\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/scipy/sparse/_sputils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmath\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_util\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnp_long\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp_ulong\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/scipy/_lib/_util.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m from scipy._lib._array_api import (Array, array_namespace, is_lazy_array,\n\u001b[0m\u001b[1;32m     15\u001b[0m                                    \u001b[0mis_numpy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxp_result_device\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m                                    xp_size, xp_result_type)\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/scipy/_lib/_array_api.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0marray_api_compat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m from scipy._lib.array_api_compat import (\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0mis_array_api_obj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mis_lazy_array\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/scipy/_lib/array_api_compat/numpy/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFinal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F403  # pyright: ignore[reportWildcardImportFromLibrary]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# from numpy import * doesn't overwrite these builtin names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numpy/testing/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_private\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverrides\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_private\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mextbuild\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_private\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_private\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_assert_valid_refcount\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_gen_alignment_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numpy/testing/_private/utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0mIS_PYSTON\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"pyston_version_info\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0mHAS_REFCOUNT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'getrefcount'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mIS_PYSTON\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m \u001b[0mBLAS_SUPPORTS_FPE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_multiarray_umath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_blas_supports_fpe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0mHAS_LAPACK64\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_umath_linalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ilp64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'numpy._core._multiarray_umath' has no attribute '_blas_supports_fpe'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MOCK T1D ENVIRONMENT"
      ],
      "metadata": {
        "id": "x3ctRdZrxgGU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "# Aggressive clean uninstall and cache purge to resolve numpy compatibility issues\n",
        "os.system('pip uninstall -y gym gymnasium numpy minari d3rlpy scipy scikit-learn 2>/dev/null')\n",
        "os.system('pip cache purge')\n",
        "\n",
        "# Install compatible versions forcefully\n",
        "os.system('pip install -q \"numpy==1.23.5\" --force-reinstall')\n",
        "os.system('pip install -q \"scipy==1.9.3\" --force-reinstall')\n",
        "os.system('pip install -q \"scikit-learn==1.2.2\" --force-reinstall')\n",
        "os.system('pip install -q d3rlpy minari gymnasium gym --force-reinstall')\n",
        "\n",
        "import gymnasium\n",
        "import numpy as np\n",
        "from typing import Optional, Tuple, Dict, Any"
      ],
      "metadata": {
        "id": "jREkFdyEzWsN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# MOCK T1D ENVIRONMENT\n",
        "# ============================================================================\n",
        "\n",
        "class MockT1DEnv:\n",
        "    \"\"\"Mock Type 1 Diabetes environment for offline RL training.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"Initialize mock environment.\"\"\"\n",
        "        self.current_glucose = 120.0\n",
        "        self.time_step = 0\n",
        "        self.max_steps = 480\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Reset the environment.\"\"\"\n",
        "        min_glucose = 100.0\n",
        "        max_glucose = 150.0\n",
        "        self.current_glucose = np.random.uniform(min_glucose, max_glucose)\n",
        "        self.time_step = 0\n",
        "        return self.current_glucose\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"Step the environment.\"\"\"\n",
        "        action = float(action)\n",
        "        baseline = 15.0\n",
        "        mean_noise = 0.0\n",
        "        std_noise = 5.0\n",
        "        noise = np.random.normal(mean_noise, std_noise)\n",
        "        factor = 0.5\n",
        "        delta = (action - baseline) * factor + noise\n",
        "        self.current_glucose = self.current_glucose + delta\n",
        "        self.current_glucose = np.clip(self.current_glucose, 40.0, 300.0)\n",
        "\n",
        "        self.time_step = self.time_step + 1\n",
        "        done = self.time_step >= self.max_steps\n",
        "\n",
        "        if self.current_glucose < 70.0:\n",
        "            reward = -1.0\n",
        "        elif self.current_glucose > 180.0:\n",
        "            reward = -0.5\n",
        "        else:\n",
        "            reward = 1.0\n",
        "\n",
        "        info = {'glucose': self.current_glucose}\n",
        "\n",
        "        return self.current_glucose, reward, done, info\n",
        "\n",
        "    def close(self):\n",
        "        \"\"\"Close environment.\"\"\"\n",
        "        pass\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# SIMGLUCOSE ENVIRONMENT WRAPPER - DIRECT INSTANTIATION (Now wraps MockT1DEnv)\n",
        "# ============================================================================\n",
        "\n",
        "class SimglucoseGymEnv(gymnasium.Env):\n",
        "    \"\"\"\n",
        "    Gymnasium-compatible wrapper for SimGlucose T1DSimEnv.\n",
        "    Now directly instantiates MockT1DEnv due to simglucose dependency issues.\n",
        "    \"\"\"\n",
        "\n",
        "    metadata = {\"render_modes\": [\"human\"], \"render_fps\": 30}\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        patient_name: str = 'adolescent#001',\n",
        "        seed: Optional[int] = None,\n",
        "        render_mode: Optional[str] = None\n",
        "    ):\n",
        "        \"\"\"Initialize the SimglucoseGymEnv.\"\"\"\n",
        "        super().__init__() # Removed seed=seed here\n",
        "\n",
        "        self.render_mode = render_mode\n",
        "        self.patient_name = patient_name\n",
        "        self._episode_steps = 0\n",
        "        self._max_episode_steps = 480\n",
        "        self._episode_rewards = []\n",
        "        self._last_obs = None\n",
        "\n",
        "        # Use MockT1DEnv instead of T1DSimEnv due to dependency conflicts\n",
        "        self.env = MockT1DEnv()\n",
        "        print(f\"OK Successfully initialized MockT1DEnv (instead of T1DSimEnv due to compatibility issues).\")\n",
        "\n",
        "        self.action_space = gymnasium.spaces.Box(\n",
        "            low=np.float32(0.0),\n",
        "            high=np.float32(30.0),\n",
        "            shape=(1,),\n",
        "            dtype=np.float32\n",
        "        )\n",
        "\n",
        "        self.observation_space = gymnasium.spaces.Box(\n",
        "            low=np.float32(0.0),\n",
        "            high=np.float32(1000.0),\n",
        "            shape=(1,),\n",
        "            dtype=np.float32\n",
        "        )\n",
        "\n",
        "        # Ensure reproducibility for internal random operations if seed is provided\n",
        "        if seed is not None:\n",
        "            np.random.seed(seed)\n",
        "\n",
        "    def step(self, action: np.ndarray) -> Tuple[np.ndarray, float, bool, bool, dict]:\n",
        "        \"\"\"Perform one step in the environment.\"\"\"\n",
        "        if isinstance(action, np.ndarray):\n",
        "            scalar_action = float(action[0]) if action.size == 1 else float(action)\n",
        "        else:\n",
        "            scalar_action = float(action)\n",
        "\n",
        "        try:\n",
        "            observation, reward, done, info = self.env.step(scalar_action)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"ERROR in step: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            raise\n",
        "\n",
        "        self._episode_steps += 1\n",
        "        self._episode_rewards.append(float(reward))\n",
        "\n",
        "        if observation is None:\n",
        "            observation = self._last_obs if self._last_obs is not None else np.array([0.0], dtype=np.float32)\n",
        "        else:\n",
        "            if not isinstance(observation, np.ndarray):\n",
        "                observation = np.array([float(observation)], dtype=np.float32)\n",
        "            else:\n",
        "                if observation.ndim == 0:\n",
        "                    observation = np.array([float(observation)], dtype=np.float32)\n",
        "                elif observation.shape == (1,):\n",
        "                    observation = observation.astype(np.float32)\n",
        "                else:\n",
        "                    observation = np.array([float(observation.flat[0])], dtype=np.float32)\n",
        "            self._last_obs = observation.copy()\n",
        "\n",
        "        truncated = self._episode_steps >= self._max_episode_steps\n",
        "\n",
        "        return observation, float(reward), bool(done), truncated, info\n",
        "\n",
        "    def reset(\n",
        "        self,\n",
        "        seed: Optional[int] = None,\n",
        "        options: Optional[dict] = None\n",
        "    ) -> Tuple[np.ndarray, dict]:\n",
        "        \"\"\"Reset the environment.\"\"\"\n",
        "        super().reset(seed=seed)\n",
        "\n",
        "        try:\n",
        "            # If a seed is provided to reset, ensure MockT1DEnv uses it for reproducibility\n",
        "            if seed is not None:\n",
        "                np.random.seed(seed)\n",
        "            observation = self.env.reset()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"ERROR in reset: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            raise\n",
        "\n",
        "        if observation is None:\n",
        "            observation = np.array([0.0], dtype=np.float32)\n",
        "        else:\n",
        "            if not isinstance(observation, np.ndarray):\n",
        "                observation = np.array([float(observation)], dtype=np.float32)\n",
        "            else:\n",
        "                if observation.ndim == 0:\n",
        "                    observation = np.array([float(observation)], dtype=np.float32)\n",
        "                elif observation.shape == (1,):\n",
        "                    observation = observation.astype(np.float32)\n",
        "                else:\n",
        "                    observation = np.array([float(observation.flat[0])], dtype=np.float32)\n",
        "\n",
        "        self._last_obs = observation.copy()\n",
        "        self._episode_steps = 0\n",
        "        self._episode_rewards = []\n",
        "\n",
        "        return observation, {}\n",
        "\n",
        "    def render(self) -> Optional[Any]:\n",
        "        \"\"\"Render the environment (if applicable).\"\"\"\n",
        "        return None\n",
        "\n",
        "    def close(self) -> None:\n",
        "        \"\"\"Close the environment and cleanup resources.\"\"\"\n",
        "        if hasattr(self, 'env'):\n",
        "            self.env.close()\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# ENVIRONMENT SETUP AND TESTING\n",
        "# ============================================================================\n",
        "\n",
        "def setup_simglucose_environment(\n",
        "    patient_name: str = 'adolescent#001',\n",
        "    seed: int = 42\n",
        ") -> SimglucoseGymEnv:\n",
        "    \"\"\"Setup and initialize a SimGlucose environment.\"\"\"\n",
        "    print(\"Initializing SimglucoseGymEnv...\")\n",
        "    env = SimglucoseGymEnv(patient_name=patient_name, seed=seed)\n",
        "    print(\"OK SimglucoseGymEnv initialized successfully!\")\n",
        "    return env\n",
        "\n",
        "\n",
        "def test_environment(env: SimglucoseGymEnv, n_steps: int = 5) -> None:\n",
        "    \"\"\"Test the environment with random actions.\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"Testing Environment with Random Actions\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    obs, info = env.reset(seed=42)\n",
        "    print(f\"\\nOK Reset successful!\")\n",
        "    print(f\"Initial Observation: {obs}\")\n",
        "\n",
        "    print(\"\\n--- Testing Steps ---\")\n",
        "    episode_rewards = []\n",
        "\n",
        "    for i in range(n_steps):\n",
        "        action = env.action_space.sample()\n",
        "        obs, reward, terminated, truncated, info = env.step(action)\n",
        "        episode_rewards.append(reward)\n",
        "\n",
        "        print(f\"Step {i + 1}: obs={obs[0]:.2f}, reward={reward:.4f}\")\n",
        "\n",
        "        if terminated or truncated:\n",
        "            print(\"  Episode ended!\")\n",
        "            break\n",
        "\n",
        "    print(f\"\\n--- Episode Summary ---\")\n",
        "    print(f\"  Total Steps: {len(episode_rewards)}\")\n",
        "    print(f\"  Total Return: {sum(episode_rewards):.4f}\")\n",
        "    print(f\"  Average Reward: {np.mean(episode_rewards):.4f}\")\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "\n",
        "\n",
        "def define_behavior_policy(observation: np.ndarray, env: SimglucoseGymEnv) -> np.ndarray:\n",
        "    \"\"\"Simple random behavior policy for data collection.\"\"\"\n",
        "    return env.action_space.sample()"
      ],
      "metadata": {
        "id": "qvUQNZFOrd51"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Collection"
      ],
      "metadata": {
        "id": "8878GPOPxsTJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# DATA COLLECTION\n",
        "# ============================================================================\n",
        "\n",
        "def setup_data_collection(\n",
        "    env: SimglucoseGymEnv,\n",
        "    num_episodes: int = 10,\n",
        "    max_steps_per_episode: int = 480,\n",
        "    dataset_name: str = None  # Add this parameter\n",
        ") -> Tuple[str, DataCollector]: # Modified to return DataCollector\n",
        "    \"\"\"Setup and prepare data collection with Minari DataCollector.\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"Setting up Minari Data Collection\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    # Use provided dataset_name or generate one\n",
        "    if dataset_name is None:\n",
        "        timestamp = time.strftime(\"%Y%m%d%H%M%S\")\n",
        "        dataset_name = f'simglucose/adolescent/random-v{timestamp}'\n",
        "\n",
        "    print(f\"\\nCreating DataCollector for dataset '{dataset_name}'...\")\n",
        "    # Initialize Minari DataCollector without dataset_id in constructor\n",
        "    data_collector = DataCollector(env, record_infos=True)\n",
        "    print(\"OK DataCollector created successfully!\")\n",
        "\n",
        "    print(f\"\\nData Collection Parameters:\")\n",
        "    print(f\"  - Dataset Name: {dataset_name}\")\n",
        "    print(f\"  - Number of Episodes: {num_episodes}\")\n",
        "    print(f\"  - Max Steps per Episode: {max_steps_per_episode}\")\n",
        "\n",
        "    return dataset_name, data_collector\n",
        "\n",
        "\n",
        "def collect_data_simple(\n",
        "    env:   SimglucoseGymEnv,\n",
        "    policy:  Callable,\n",
        "    num_episodes:  int = 10,\n",
        "    max_steps_per_episode: int = 480,\n",
        "    dataset_name: str = 'simglucose-adolescent-random-v0',\n",
        "    verbose: bool = True\n",
        "):\n",
        "    \"\"\"Collect trajectory data and return as dictionary.\"\"\"\n",
        "    if verbose:\n",
        "        print(f\"\\nStarting data collection for {num_episodes} episodes...\")\n",
        "        start_time = time.time()\n",
        "\n",
        "    episodes_data = []\n",
        "\n",
        "    for episode_num in range(num_episodes):\n",
        "        observations = []\n",
        "        actions = []\n",
        "        rewards = []\n",
        "        terminations = []\n",
        "        truncations = []\n",
        "\n",
        "        obs, _ = env.reset()\n",
        "\n",
        "        for step in range(max_steps_per_episode):\n",
        "            observations.append(obs.  copy())\n",
        "            action = policy(obs, env)\n",
        "            actions.append(action)\n",
        "\n",
        "            obs, reward, terminated, truncated, info = env.step(action)\n",
        "            rewards.append(reward)\n",
        "            terminations.append(terminated)\n",
        "            truncations.append(truncated)\n",
        "\n",
        "            if terminated or truncated:\n",
        "                break\n",
        "\n",
        "        episode_dict = {\n",
        "            'observations': np.array(observations),\n",
        "            'actions': np.array(actions),\n",
        "            # ✅ Reshape rewards to 2D:  (n_steps, 1)\n",
        "            'rewards': np.array(rewards, dtype=np.float32).reshape(-1, 1),\n",
        "            'terminations': np.array(terminations),\n",
        "            'truncations': np.  array(truncations),\n",
        "        }\n",
        "        episodes_data.append(episode_dict)\n",
        "\n",
        "    elapsed_time = time.time() - start_time if verbose else None\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"OK Data collection complete! (took {elapsed_time:.2f}s)\")\n",
        "        print(f\"Collected {len(episodes_data)} episodes\")\n",
        "        print(f\"   Total transitions: {sum(len(ep['rewards']) for ep in episodes_data)}\")\n",
        "\n",
        "    return episodes_data\n",
        "\n",
        "\n",
        "def collect_data_and_save(\n",
        "    num_episodes: int = 10,\n",
        "    max_steps_per_episode: int = 480,\n",
        "    patient_name: str = 'adolescent#001',\n",
        "    policy_type: str = 'random'\n",
        "):\n",
        "    \"\"\"Complete SimGlucose environment setup and data collection pipeline.\"\"\"\n",
        "    print(\"=\" * 80)\n",
        "    print(\"SimGlucose Environment Setup and Data Collection\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    env = setup_simglucose_environment(patient_name=patient_name, seed=42)\n",
        "    test_environment(env, n_steps=5)\n",
        "\n",
        "    policy = define_behavior_policy\n",
        "    print(\"\\nOK Using random policy\")\n",
        "\n",
        "    episodes_data = collect_data_simple(\n",
        "        env=env,\n",
        "        policy=policy,\n",
        "        num_episodes=num_episodes,\n",
        "        max_steps_per_episode=max_steps_per_episode,\n",
        "        dataset_name=None,\n",
        "        verbose=True\n",
        "    )\n",
        "\n",
        "    env.close()\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"OK Data Collection Pipeline Complete!\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    return episodes_data"
      ],
      "metadata": {
        "id": "tjS3du7-rhlE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "8b963d09-7b83-4d19-c5a4-3dc1b033931b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'DataCollector' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1327563975.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mmax_steps_per_episode\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m480\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mdataset_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# Add this parameter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m ) -> Tuple[str, DataCollector]: # Modified to return DataCollector\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;34m\"\"\"Setup and prepare data collection with Minari DataCollector.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"=\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m80\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'DataCollector' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DATASET MANAGEMENT"
      ],
      "metadata": {
        "id": "V2vj1GPqx67O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# MINARI DATASET MANAGEMENT & REPLAY BUFFER MANAGEMENT\n",
        "# ============================================================================\n",
        "\n",
        "class MinariDatasetLoader:\n",
        "    \"\"\"Handler for loading and managing Minari datasets.\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def load_dataset(dataset_name: str = 'simglucose-adolescent-random-v0') -> MinariDataset:\n",
        "        \"\"\"Load a Minari dataset.\"\"\"\n",
        "        print(f\"\\n{'='*80}\")\n",
        "        print(f\"Loading Minari Dataset:  {dataset_name}\")\n",
        "        print(f\"{'='*80}\")\n",
        "\n",
        "        try:\n",
        "            # Try loading the exact dataset_name passed, which might be a pre-existing one\n",
        "            # or a freshly generated unique one from collect_data_and_save.\n",
        "            dataset = load_dataset(dataset_name, download=False)\n",
        "            print(f\"OK Dataset loaded successfully!\")\n",
        "            print(f\"  - Total episodes: {len(dataset.episodes)}\")\n",
        "            total_transitions = sum(ep.transition_count for ep in dataset.episodes)\n",
        "            print(f\"  - Total transitions: {total_transitions}\")\n",
        "            return dataset\n",
        "\n",
        "        except FileNotFoundError:\n",
        "            print(f\"Dataset '{dataset_name}' not found locally. Collecting new data...\")\n",
        "            # If not found, generate a unique name and collect new data\n",
        "            timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
        "            unique_dataset_name = f'simglucose-adolescent-random-v0-{timestamp}'\n",
        "            dataset = collect_data_and_save(\n",
        "                num_episodes=10,\n",
        "                max_steps_per_episode=480,\n",
        "                patient_name='adolescent#001',\n",
        "                policy_type='random'\n",
        "            )\n",
        "            return dataset\n",
        "\n",
        "    @staticmethod\n",
        "    def dataset_statistics(dataset: MinariDataset) -> Dict[str, Any]:\n",
        "        \"\"\"Compute statistics about the dataset.\"\"\"\n",
        "        episodes = dataset.episodes\n",
        "        episode_returns = []\n",
        "        episode_lengths = []\n",
        "        rewards_list = []\n",
        "\n",
        "        for episode in episodes:\n",
        "            episode_reward = 0.0\n",
        "            for i in range(episode.transition_count):\n",
        "                transition = episode[i]\n",
        "                episode_reward += transition.reward\n",
        "                rewards_list.append(transition.reward)\n",
        "            episode_returns.append(episode_reward)\n",
        "            episode_lengths.append(episode.transition_count)\n",
        "\n",
        "        stats = {\n",
        "            'num_episodes': len(episodes),\n",
        "            'total_transitions': sum(episode_lengths),\n",
        "            'mean_episode_return': float(np.mean(episode_returns)),\n",
        "            'std_episode_return': float(np.std(episode_returns)),\n",
        "            'max_episode_return': float(np.max(episode_returns)),\n",
        "            'min_episode_return': float(np.min(episode_returns)),\n",
        "            'mean_episode_length': float(np.mean(episode_lengths)),\n",
        "            'mean_reward': float(np.mean(rewards_list)),\n",
        "            'std_reward': float(np.std(rewards_list)),\n",
        "        }\n",
        "        return stats\n",
        "\n",
        "    @staticmethod\n",
        "    def print_statistics(stats: Dict[str, Any]) -> None:\n",
        "        \"\"\"Print dataset statistics.\"\"\"\n",
        "        print(f\"\\nDataset Statistics:\")\n",
        "        print(f\"  Episodes: {stats['num_episodes']}\")\n",
        "        print(f\"  Total Transitions: {stats['total_transitions']}\")\n",
        "        print(f\"  Mean Episode Return: {stats['mean_episode_return']:.4f} +/- {stats['std_episode_return']:.4f}\")\n",
        "        print(f\"  Mean Episode Length: {stats['mean_episode_length']:.1f}\")\n",
        "        print(f\"  Mean Reward: {stats['mean_reward']:.4f} +/- {stats['std_reward']:.4f}\")\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# REPLAY BUFFER MANAGEMENT\n",
        "# ============================================================================\n",
        "\n",
        "class ReplayBufferManager:\n",
        "    \"\"\"Handler for converting Minari datasets to d3rlpy ReplayBuffer.\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def create_replay_buffer(dataset: MinariDataset) -> Any:\n",
        "        \"\"\"Convert Minari dataset to d3rlpy ReplayBuffer.\"\"\"\n",
        "        print(f\"\\n{'='*80}\")\n",
        "        print(\"Converting Minari Dataset to d3rlpy ReplayBuffer\")\n",
        "        print(f\"{'='*80}\")\n",
        "\n",
        "        try:\n",
        "            replay_buffer = d3rlpy.dataset.create_replay_buffer(\n",
        "                episodes=dataset.episodes\n",
        "            )\n",
        "            print(f\"OK ReplayBuffer created successfully!\")\n",
        "            print(f\"  - Size: {len(replay_buffer)} transitions\")\n",
        "            return replay_buffer\n",
        "        except Exception as e:\n",
        "            print(f\"ERROR creating replay buffer: {e}\")\n",
        "            raise"
      ],
      "metadata": {
        "id": "Jk95HYg8x4ae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OFFLINE RL ALGORITHM CONFIGURATION & TRAINING\n"
      ],
      "metadata": {
        "id": "RHynWrH9yB-2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# OFFLINE RL ALGORITHM CONFIGURATION\n",
        "# ============================================================================\n",
        "\n",
        "class OfflineRLAlgorithm:\n",
        "    \"\"\"Factory for creating offline RL algorithms.\"\"\"\n",
        "\n",
        "    ALGORITHM_CONFIGS = {\n",
        "        'cql': {\n",
        "            'name': 'Conservative Q-Learning (CQL)',\n",
        "            'description': 'Best for general offline RL',\n",
        "            'config_class': d3rlpy.algos.CQLConfig,\n",
        "            'params': {\n",
        "                'actor_learning_rate': 1e-4,\n",
        "                'critic_learning_rate': 3e-4,\n",
        "                'batch_size': 256,\n",
        "                'gamma': 0.99,\n",
        "                'tau': 5e-3,\n",
        "                'alpha_learning_rate': 1e-4,\n",
        "                'conservative_weight': 10.0,\n",
        "                'n_action_samples': 10,\n",
        "            }\n",
        "        },\n",
        "        'iql': {\n",
        "            'name':  'Implicit Q-Learning (IQL)',\n",
        "            'description': 'Avoids querying unseen actions',\n",
        "            'config_class':  d3rlpy.algos.IQLConfig,\n",
        "            'params': {\n",
        "                'actor_learning_rate': 3e-4,\n",
        "                'critic_learning_rate': 3e-4,\n",
        "                'batch_size': 256,\n",
        "                'gamma': 0.99,\n",
        "                'tau': 5e-3,\n",
        "                'expectile':  0.7,\n",
        "                'weight_temp': 3.0,\n",
        "                'max_weight': 100.0,\n",
        "            }\n",
        "        },\n",
        "        'bc': {\n",
        "            'name': 'Behavioral Cloning (BC)',\n",
        "            'description': 'Simple imitation learning baseline',\n",
        "            'config_class':  d3rlpy.algos.BCConfig,\n",
        "            'params': {\n",
        "                'learning_rate': 1e-4,\n",
        "                'batch_size': 256,\n",
        "            }\n",
        "        },\n",
        "    }\n",
        "\n",
        "    @classmethod\n",
        "    def list_algorithms(cls) -> None:\n",
        "        \"\"\"List available algorithms.\"\"\"\n",
        "        print(f\"\\n{'='*80}\")\n",
        "        print(\"Available Offline RL Algorithms\")\n",
        "        print(f\"{'='*80}\")\n",
        "        for algo_type, config in cls.ALGORITHM_CONFIGS.items():\n",
        "            print(f\"\\n  {algo_type.upper()}:\\n    Name: {config['name']}\\n    Description: {config['description']}\")\n",
        "\n",
        "    @classmethod\n",
        "    def create(cls, algo_type: str = 'cql', device: str = 'cpu:0', **custom_params):\n",
        "        \"\"\"Create an offline RL algorithm.\"\"\"\n",
        "        algo_type = algo_type.lower()\n",
        "\n",
        "        if algo_type not in cls.ALGORITHM_CONFIGS:\n",
        "            raise ValueError(f\"Unknown algorithm:  {algo_type}\")\n",
        "\n",
        "        config_info = cls.ALGORITHM_CONFIGS[algo_type]\n",
        "        params = config_info['params'].copy()\n",
        "        params.update(custom_params)\n",
        "\n",
        "        print(f\"\\n{'='*80}\")\n",
        "        print(f\"Creating {config_info['name']}\")\n",
        "        print(f\"{'='*80}\")\n",
        "        print(f\"Device: {device}\")\n",
        "        print(f\"\\nHyperparameters:\")\n",
        "        for key, value in params.items():\n",
        "            print(f\"  - {key}: {value}\")\n",
        "\n",
        "        config = config_info['config_class'](**params)\n",
        "        algo = config.create(device=device)\n",
        "\n",
        "        print(f\"OK {config_info['name']} created successfully!\")\n",
        "        return algo\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# TRAINING\n",
        "# ============================================================================\n",
        "\n",
        "class OfflineRLTrainer:\n",
        "    \"\"\"Trainer for offline RL algorithms.\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def train(\n",
        "        algo,\n",
        "        replay_buffer,\n",
        "        n_steps:  int = 50000,\n",
        "        save_interval: int = 5000,\n",
        "        verbose: bool = True\n",
        "    ) -> Dict[str, Any]:\n",
        "        \"\"\"Train offline RL algorithm.\"\"\"\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"Starting Offline RL Training\")\n",
        "        print(\"=\"*80)\n",
        "        print(\"Training Configuration:\")\n",
        "        print(f\"  - Algorithm: {algo.__class__.__name__}\")\n",
        "        print(f\"  - Total Steps: {n_steps}\")\n",
        "        # ✅ Use . size() instead of len()\n",
        "        print(f\"  - Replay Buffer Size: {replay_buffer.size()}\")\n",
        "        print()\n",
        "\n",
        "        start_time = time.time()\n",
        "\n",
        "        try:\n",
        "            # Train the algorithm\n",
        "            algo.fit(\n",
        "                replay_buffer,\n",
        "                n_steps=n_steps,\n",
        "                show_progress=verbose\n",
        "            )\n",
        "\n",
        "            training_time = time.time() - start_time\n",
        "\n",
        "            if verbose:\n",
        "                print(f\"\\nOK Training complete!\")\n",
        "                print(f\"  - Training Time: {training_time:.2f}s\") # Fixed f-string format\n",
        "                print(f\"  - Steps per Second: {n_steps / training_time:.2f}\")\n",
        "\n",
        "            return {\n",
        "                'n_steps': n_steps,\n",
        "                'training_time': training_time,\n",
        "                'steps_per_second': n_steps / training_time\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            if verbose:\n",
        "                print(f\"ERROR during training: {e}\")\n",
        "            raise\n"
      ],
      "metadata": {
        "id": "sKeZWyvvrhoL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EVALUATION & MODEL PERSISTENCE"
      ],
      "metadata": {
        "id": "44WfTjaeyRxp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ============================================================================\n",
        "# EVALUATION\n",
        "# ============================================================================\n",
        "\n",
        "class PolicyEvaluator:\n",
        "    \"\"\"Handler for evaluating trained policies.\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def evaluate(\n",
        "        algo,\n",
        "        env,\n",
        "        n_episodes: int = 5,\n",
        "        max_steps:  Optional[int] = None,\n",
        "        verbose: bool = True\n",
        "    ) -> Dict[str, float]:\n",
        "        \"\"\"Evaluate a trained policy.\"\"\"\n",
        "        if verbose:\n",
        "            print(f\"\\n{'='*80}\")\n",
        "            print(f\"Evaluating Policy ({n_episodes} episodes)\")\n",
        "            print(f\"{'='*80}\")\n",
        "\n",
        "        episode_returns = []\n",
        "        episode_lengths = []\n",
        "\n",
        "        for episode_num in range(n_episodes):\n",
        "            obs, _ = env.reset()\n",
        "            total_return = 0.0\n",
        "            steps = 0\n",
        "\n",
        "            done = False\n",
        "            while not done:\n",
        "                action = algo.predict(np.expand_dims(obs, axis=0))[0]\n",
        "                obs, reward, terminated, truncated, _ = env.step(action)\n",
        "                total_return += reward\n",
        "                steps += 1\n",
        "\n",
        "                done = terminated or truncated\n",
        "                if max_steps and steps >= max_steps:\n",
        "                    done = True\n",
        "\n",
        "            episode_returns.append(total_return)\n",
        "            episode_lengths.append(steps)\n",
        "\n",
        "            if verbose:\n",
        "                print(f\"  Episode {episode_num + 1:3d}: Return = {total_return:8.2f}, Steps = {steps:3d}\")\n",
        "\n",
        "        mean_return = float(np.mean(episode_returns))\n",
        "        std_return = float(np.std(episode_returns))\n",
        "        mean_length = float(np.mean(episode_lengths))\n",
        "\n",
        "        stats = {\n",
        "            'mean_return': mean_return,\n",
        "            'std_return': std_return,\n",
        "            'max_return': float(np.max(episode_returns)),\n",
        "            'min_return': float(np.min(episode_returns)),\n",
        "            'mean_episode_length': mean_length,\n",
        "        }\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"\\nOK Evaluation Complete!\")\n",
        "            print(f\"  - Mean Return: {mean_return:.4f} +/- {std_return:.4f}\")\n",
        "\n",
        "        return stats\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# MODEL PERSISTENCE\n",
        "# ============================================================================\n",
        "\n",
        "class ModelManager:\n",
        "    \"\"\"Handler for saving and loading models.\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def save_model(algo, save_path: str = \"./offline_rl_model\") -> None:\n",
        "        \"\"\"Save a trained model.\"\"\"\n",
        "        print(f\"\\nSaving model to {save_path}...\")\n",
        "        algo.save_model(save_path)\n",
        "        print(f\"OK Model saved successfully!\")\n",
        "\n",
        "    @staticmethod\n",
        "    def load_model(\n",
        "        algo_type: str,\n",
        "        save_path: str = \"./offline_rl_model\",\n",
        "        device: str = \"cpu:0\"\n",
        "    ):\n",
        "        \"\"\"Load a trained model.\"\"\"\n",
        "        print(f\"\\nLoading model from {save_path}...\")\n",
        "        config_class = OfflineRLAlgorithm.ALGORITHM_CONFIGS[algo_type.lower()]['config_class']\n",
        "        algo = config_class().create(device=device)\n",
        "        algo.load_model(save_path)\n",
        "        print(f\"OK Model loaded successfully!\")\n",
        "        return algo"
      ],
      "metadata": {
        "id": "gLExY9uer_OY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# COMPLETE PIPELINE & MAIN EXECUTION"
      ],
      "metadata": {
        "id": "dTGcdSvIycMw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# COMPLETE PIPELINE\n",
        "# ============================================================================\n",
        "\n",
        "def main(\n",
        "    episodes_data:      List[Dict],  # Accept episodes data directly\n",
        "    algo_type:  str = 'cql',\n",
        "    n_steps:  int = 100000,\n",
        "    n_eval_episodes: int = 5,\n",
        "    device:  str = 'cpu:  0',\n",
        "    save_model_flag:  bool = True\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"Complete offline RL pipeline.\"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"OFFLINE DEEP REINFORCEMENT LEARNING PIPELINE\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    OfflineRLAlgorithm.  list_algorithms()\n",
        "\n",
        "    # Create replay buffer directly from episodes\n",
        "    try:\n",
        "        # ✅ Convert dict episodes to d3rlpy Episode objects\n",
        "        from d3rlpy.dataset import Episode\n",
        "\n",
        "        d3rlpy_episodes = []\n",
        "        for episode_dict in episodes_data:\n",
        "            # ✅ Episode requires (observations, actions, rewards, terminated:   bool)\n",
        "            # terminated should be True if episode ended, False otherwise\n",
        "            episode = Episode(\n",
        "                observations=episode_dict['observations'],\n",
        "                actions=episode_dict['actions'],\n",
        "                rewards=episode_dict['rewards'],\n",
        "                terminated=True  # ✅ Mark as terminated (episode finished)\n",
        "            )\n",
        "            d3rlpy_episodes.append(episode)\n",
        "\n",
        "        # Now create replay buffer with proper Episode objects\n",
        "        replay_buffer = d3rlpy.dataset.create_fifo_replay_buffer(\n",
        "            episodes=d3rlpy_episodes,\n",
        "            limit=1000000  # 1 million transitions max\n",
        "        )\n",
        "        print(f\"\\nOK ReplayBuffer created successfully!\")\n",
        "        # ✅ Use size() method or get total_transitions\n",
        "        buffer_size = replay_buffer.size()\n",
        "        print(f\"  - Size: {buffer_size} transitions\")\n",
        "        results['replay_buffer_size'] = buffer_size\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR creating replay buffer: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        raise\n",
        "\n",
        "    algo = OfflineRLAlgorithm.  create(algo_type=algo_type, device=device)\n",
        "    results['algorithm'] = algo_type\n",
        "\n",
        "    training_stats = OfflineRLTrainer. train(\n",
        "        algo=algo,\n",
        "        replay_buffer=replay_buffer,\n",
        "        n_steps=n_steps,\n",
        "        save_interval=max(1000, n_steps // 10),\n",
        "        verbose=True\n",
        "    )\n",
        "    results['training_stats'] = training_stats\n",
        "\n",
        "    try:\n",
        "        from gymnasium.  wrappers import TimeLimit\n",
        "        eval_env = TimeLimit(\n",
        "            SimglucoseGymEnv(patient_name='adolescent#001', seed=42),\n",
        "            max_episode_steps=480\n",
        "        )\n",
        "\n",
        "        eval_stats = PolicyEvaluator.evaluate(\n",
        "            algo=algo,\n",
        "            env=eval_env,\n",
        "            n_episodes=n_eval_episodes,\n",
        "            max_steps=480,\n",
        "            verbose=True\n",
        "        )\n",
        "        results['evaluation_stats'] = eval_stats\n",
        "        eval_env.close()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nWARNING Error during evaluation: {e}\")\n",
        "\n",
        "    if save_model_flag:\n",
        "        model_path = f\"./offline_rl_{algo_type}_model\"\n",
        "        ModelManager.save_model(algo, save_path=model_path)\n",
        "        results['model_path'] = model_path\n",
        "\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(\"TRAINING SUMMARY\")\n",
        "    print(f\"{'='*80}\")\n",
        "    print(f\"OK Pipeline Complete!\")\n",
        "    print(f\"\\nResults:\")\n",
        "    print(f\"  - Algorithm: {results['algorithm']}\")\n",
        "    print(f\"  - Training Steps: {results['training_stats']['n_steps']}\")\n",
        "    print(f\"  - Training Time: {results['training_stats']['training_time']:.2f}s\")\n",
        "    print(f\"  - Replay Buffer Size: {results['replay_buffer_size']}\")\n",
        "\n",
        "    if 'evaluation_stats' in results:\n",
        "        eval_stats = results['evaluation_stats']\n",
        "        print(f\"  - Evaluation Mean Return: {eval_stats['mean_return']:.4f} +/- {eval_stats['std_return']:.4f}\")\n",
        "\n",
        "    if 'model_path' in results:\n",
        "        print(f\"  - Model Saved:   {results['model_path']}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# MAIN EXECUTION\n",
        "# ============================================================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"=\" * 70)\n",
        "    print(\"PHASE 1: ENVIRONMENT VERIFICATION\")\n",
        "    print(\"=\" * 70)\n",
        "    # ...  Phase 1 code unchanged ...\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"PHASE 2: DATA COLLECTION\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    episodes_data = None\n",
        "    try:\n",
        "        episodes_data = collect_data_and_save(\n",
        "            num_episodes=10,\n",
        "            max_steps_per_episode=480,\n",
        "            patient_name='adolescent#001',\n",
        "            policy_type='random'\n",
        "        )\n",
        "        print(f\"OK Collected {len(episodes_data)} episodes\")\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR during data collection: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"PHASE 3: OFFLINE RL TRAINING\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    try:\n",
        "        if episodes_data is not None:\n",
        "            results = main(\n",
        "                episodes_data=episodes_data,\n",
        "                algo_type='cql',\n",
        "                n_steps=50000,\n",
        "                n_eval_episodes=3,\n",
        "                device='cpu:0',\n",
        "                save_model_flag=True\n",
        "            )\n",
        "            print(\"\\n\" + \"=\" * 80)\n",
        "            print(\"OK TRAINING COMPLETE!\")\n",
        "            print(\"=\" * 80)\n",
        "        else:\n",
        "            print(\"WARNING: No episodes collected.  Skipping training.\")\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR during training: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"OK COMPLETE OFFLINE DRL PIPELINE FINISHED!\")\n",
        "    print(\"=\" * 80)"
      ],
      "metadata": {
        "id": "lCjyP40RsI52"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOp9aSqgoPyqAeT+9qqtFmu",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}