{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b71ba3-3007-41c4-857c-490fc7469d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import sys\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 1: Apply Compatibility Patches\n",
    "# ============================================================================\n",
    "\n",
    "def apply_compatibility_patches() -> None:\n",
    "    \"\"\"Apply necessary compatibility patches for gym/gymnasium compatibility.\"\"\"\n",
    "    print(\"Applying compatibility patches...\")\n",
    "    print(\"OK Compatibility patches applied\\n\")\n",
    "\n",
    "apply_compatibility_patches()\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 2: Import Main Libraries\n",
    "# ============================================================================\n",
    "\n",
    "print(\"Importing libraries...\")\n",
    "\n",
    "# Clean uninstall\n",
    "! pip uninstall -y gym gymnasium numpy minari d3rlpy scipy scikit-learn 2>/dev/null\n",
    "\n",
    "# Install ONLY compatible versions\n",
    "!pip install -q 'numpy==1.26.4'\n",
    "!pip install -q 'scipy>=1.6.0'\n",
    "!pip install -q 'scikit-learn>=1.5.0,<1.8'\n",
    "!pip install -q d3rlpy\n",
    "!pip install -q minari\n",
    "\n",
    "# NOW import the packages AFTER they're installed\n",
    "import gymnasium\n",
    "import gym\n",
    "from typing import Optional, Tuple, Dict, List, Any, Callable\n",
    "import time\n",
    "import numpy as np\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "try:\n",
    "    import d3rlpy\n",
    "    import minari\n",
    "    print(\"OK All imports successful\\n\")\n",
    "    print(f\"DEBUG:  Minari version after (re)install: {minari.__version__}\")\n",
    "    print(f\"DEBUG: d3rlpy version:  {d3rlpy.__version__}\")\n",
    "except Exception as e: \n",
    "    print(f\"ERROR Import error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e860c346-2b52-4521-8751-610a410cdc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# MOCK T1D ENVIRONMENT (Copied from previous cell to make this cell self-contained)\n",
    "# ============================================================================\n",
    "\n",
    "class MockT1DEnv:\n",
    "    \"\"\"Mock Type 1 Diabetes environment for offline RL training.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize mock environment.\"\"\"\n",
    "        self.current_glucose = 120.0\n",
    "        self.time_step = 0\n",
    "        self.max_steps = 480\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the environment.\"\"\"\n",
    "        min_glucose = 100.0\n",
    "        max_glucose = 150.0\n",
    "        self.current_glucose = np.random.uniform(min_glucose, max_glucose)\n",
    "        self.time_step = 0\n",
    "        return self.current_glucose\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Step the environment.\"\"\"\n",
    "        action = float(action)\n",
    "        baseline = 15.0\n",
    "        mean_noise = 0.0\n",
    "        std_noise = 5.0\n",
    "        noise = np.random.normal(mean_noise, std_noise)\n",
    "        factor = 0.5\n",
    "        delta = (action - baseline) * factor + noise\n",
    "        self.current_glucose = self.current_glucose + delta\n",
    "        self.current_glucose = np.clip(self.current_glucose, 40.0, 300.0)\n",
    "\n",
    "        self.time_step = self.time_step + 1\n",
    "        done = self.time_step >= self.max_steps\n",
    "\n",
    "        if self.current_glucose < 70.0:\n",
    "            reward = -1.0\n",
    "        elif self.current_glucose > 180.0:\n",
    "            reward = -0.5\n",
    "        else:\n",
    "            reward = 1.0\n",
    "\n",
    "        info = {'glucose': self.current_glucose}\n",
    "\n",
    "        return self.current_glucose, reward, done, info\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"Close environment.\"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SIMGLUCOSE ENVIRONMENT WRAPPER - DIRECT INSTANTIATION (Now wraps MockT1DEnv)\n",
    "# ============================================================================\n",
    "\n",
    "class SimglucoseGymEnv(gymnasium.Env):\n",
    "    \"\"\"\n",
    "    Gymnasium-compatible wrapper for SimGlucose T1DSimEnv.\n",
    "    Now directly instantiates MockT1DEnv due to simglucose dependency issues.\n",
    "    \"\"\"\n",
    "\n",
    "    metadata = {\"render_modes\": [\"human\"], \"render_fps\": 30}\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        patient_name: str = 'adolescent#001',\n",
    "        seed: Optional[int] = None,\n",
    "        render_mode: Optional[str] = None\n",
    "    ):\n",
    "        \"\"\"Initialize the SimglucoseGymEnv.\"\"\"\n",
    "        super().__init__() # Removed seed=seed here\n",
    "\n",
    "        self.render_mode = render_mode\n",
    "        self.patient_name = patient_name\n",
    "        self._episode_steps = 0\n",
    "        self._max_episode_steps = 480\n",
    "        self._episode_rewards = []\n",
    "        self._last_obs = None\n",
    "\n",
    "        # Use MockT1DEnv instead of T1DSimEnv due to dependency conflicts\n",
    "        self.env = MockT1DEnv()\n",
    "        print(f\"OK Successfully initialized MockT1DEnv (instead of T1DSimEnv due to compatibility issues).\")\n",
    "\n",
    "        self.action_space = gymnasium.spaces.Box(\n",
    "            low=np.float32(0.0),\n",
    "            high=np.float32(30.0),\n",
    "            shape=(1,),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "\n",
    "        self.observation_space = gymnasium.spaces.Box(\n",
    "            low=np.float32(0.0),\n",
    "            high=np.float32(1000.0),\n",
    "            shape=(1,),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "\n",
    "        # Ensure reproducibility for internal random operations if seed is provided\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "\n",
    "    def step(self, action: np.ndarray) -> Tuple[np.ndarray, float, bool, bool, dict]:\n",
    "        \"\"\"Perform one step in the environment.\"\"\"\n",
    "        if isinstance(action, np.ndarray):\n",
    "            scalar_action = float(action[0]) if action.size == 1 else float(action)\n",
    "        else:\n",
    "            scalar_action = float(action)\n",
    "\n",
    "        try:\n",
    "            observation, reward, done, info = self.env.step(scalar_action)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR in step: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            raise\n",
    "\n",
    "        self._episode_steps += 1\n",
    "        self._episode_rewards.append(float(reward))\n",
    "\n",
    "        if observation is None:\n",
    "            observation = self._last_obs if self._last_obs is not None else np.array([0.0], dtype=np.float32)\n",
    "        else:\n",
    "            if not isinstance(observation, np.ndarray):\n",
    "                observation = np.array([float(observation)], dtype=np.float32)\n",
    "            else:\n",
    "                if observation.ndim == 0:\n",
    "                    observation = np.array([float(observation)], dtype=np.float32)\n",
    "                elif observation.shape == (1,):\n",
    "                    observation = observation.astype(np.float32)\n",
    "                else:\n",
    "                    observation = np.array([float(observation.flat[0])], dtype=np.float32)\n",
    "            self._last_obs = observation.copy()\n",
    "\n",
    "        truncated = self._episode_steps >= self._max_episode_steps\n",
    "\n",
    "        return observation, float(reward), bool(done), truncated, info\n",
    "\n",
    "    def reset(\n",
    "        self,\n",
    "        seed: Optional[int] = None,\n",
    "        options: Optional[dict] = None\n",
    "    ) -> Tuple[np.ndarray, dict]:\n",
    "        \"\"\"Reset the environment.\"\"\"\n",
    "        super().reset(seed=seed)\n",
    "\n",
    "        try:\n",
    "            # If a seed is provided to reset, ensure MockT1DEnv uses it for reproducibility\n",
    "            if seed is not None:\n",
    "                np.random.seed(seed)\n",
    "            observation = self.env.reset()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR in reset: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            raise\n",
    "\n",
    "        if observation is None:\n",
    "            observation = np.array([0.0], dtype=np.float32)\n",
    "        else:\n",
    "            if not isinstance(observation, np.ndarray):\n",
    "                observation = np.array([float(observation)], dtype=np.float32)\n",
    "            else:\n",
    "                if observation.ndim == 0:\n",
    "                    observation = np.array([float(observation)], dtype=np.float32)\n",
    "                elif observation.shape == (1,):\n",
    "                    observation = observation.astype(np.float32)\n",
    "                else:\n",
    "                    observation = np.array([float(observation.flat[0])], dtype=np.float32)\n",
    "\n",
    "        self._last_obs = observation.copy()\n",
    "        self._episode_steps = 0\n",
    "        self._episode_rewards = []\n",
    "\n",
    "        return observation, {}\n",
    "\n",
    "    def render(self) -> Optional[Any]:\n",
    "        \"\"\"Render the environment (if applicable).\"\"\"\n",
    "        return None\n",
    "\n",
    "    def close(self) -> None:\n",
    "        \"\"\"Close the environment and cleanup resources.\"\"\"\n",
    "        if hasattr(self, 'env'):\n",
    "            self.env.close()\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# ENVIRONMENT SETUP AND TESTING\n",
    "# ============================================================================\n",
    "\n",
    "def setup_simglucose_environment(\n",
    "    patient_name: str = 'adolescent#001',\n",
    "    seed: int = 42\n",
    ") -> SimglucoseGymEnv:\n",
    "    \"\"\"Setup and initialize a SimGlucose environment.\"\"\"\n",
    "    print(\"Initializing SimglucoseGymEnv...\")\n",
    "    env = SimglucoseGymEnv(patient_name=patient_name, seed=seed)\n",
    "    print(\"OK SimglucoseGymEnv initialized successfully!\")\n",
    "    return env\n",
    "\n",
    "\n",
    "def test_environment(env: SimglucoseGymEnv, n_steps: int = 5) -> None:\n",
    "    \"\"\"Test the environment with random actions.\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"Testing Environment with Random Actions\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    obs, info = env.reset(seed=42)\n",
    "    print(f\"\\nOK Reset successful!\")\n",
    "    print(f\"Initial Observation: {obs}\")\n",
    "\n",
    "    print(\"\\n--- Testing Steps ---\")\n",
    "    episode_rewards = []\n",
    "\n",
    "    for i in range(n_steps):\n",
    "        action = env.action_space.sample()\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        episode_rewards.append(reward)\n",
    "\n",
    "        print(f\"Step {i + 1}: obs={obs[0]:.2f}, reward={reward:.4f}\")\n",
    "\n",
    "        if terminated or truncated:\n",
    "            print(\"  Episode ended!\")\n",
    "            break\n",
    "\n",
    "    print(f\"\\n--- Episode Summary ---\")\n",
    "    print(f\"  Total Steps: {len(episode_rewards)}\")\n",
    "    print(f\"  Total Return: {sum(episode_rewards):.4f}\")\n",
    "    print(f\"  Average Reward: {np.mean(episode_rewards):.4f}\")\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "\n",
    "\n",
    "def define_behavior_policy(observation: np.ndarray, env: SimglucoseGymEnv) -> np.ndarray:\n",
    "    \"\"\"Simple random behavior policy for data collection.\"\"\"\n",
    "    return env.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf24c41d-5d26-44b7-8741-c5659314b297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ENVIRONMENT INITIALIZATION\n",
      "================================================================================\n",
      "Patient:  adolescent#001\n",
      "Seed: 42\n",
      "OK Successfully initialized MockT1DEnv\n",
      "OK SimglucoseGymEnv initialized successfully!\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "ENVIRONMENT TEST\n",
      "================================================================================\n",
      "✅ Reset successful!\n",
      "Initial Observation: [118.727005]\n",
      "Observation shape: (1,)\n",
      "Observation dtype: float32\n",
      "\n",
      "--- Testing Steps ---\n",
      "Step 1:\n",
      "  Action: 22.04 | Glucose: 116.69 | Reward: 1.0000\n",
      "Step 2:\n",
      "  Action: 20.32 | Glucose: 120.94 | Reward: 1.0000\n",
      "Step 3:\n",
      "  Action: 17.55 | Glucose: 123.61 | Reward: 1.0000\n",
      "Step 4:\n",
      "  Action: 6.26 | Glucose: 124.29 | Reward: 1.0000\n",
      "Step 5:\n",
      "  Action: 10.62 | Glucose: 119.20 | Reward: 1.0000\n",
      "\n",
      "--- Episode Summary ---\n",
      "  Total Steps: 5\n",
      "  Total Return: 5.0000\n",
      "  Average Reward: 1.0000\n",
      "  Mean Glucose: 120.95 mg/dL\n",
      "  Glucose Range: [116.69, 124.29]\n",
      "================================================================================\n",
      "\n",
      "✅ Environment test completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# MOCK T1D ENVIRONMENT\n",
    "# ============================================================================\n",
    "\n",
    "class MockT1DEnv: \n",
    "    \"\"\"Mock Type 1 Diabetes environment for offline RL training.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize mock environment.\"\"\"\n",
    "        self. current_glucose = 120.0\n",
    "        self.time_step = 0\n",
    "        self. max_steps = 480\n",
    "\n",
    "    def reset(self) -> float:\n",
    "        \"\"\"Reset the environment and return initial observation.\"\"\"\n",
    "        min_glucose = 100.0\n",
    "        max_glucose = 150.0\n",
    "        self.current_glucose = np.random.uniform(min_glucose, max_glucose)\n",
    "        self.time_step = 0\n",
    "        return self.current_glucose\n",
    "\n",
    "    def step(self, action: float) -> Tuple[float, float, bool, Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Step the environment. \n",
    "        \n",
    "        Args:\n",
    "            action:  Insulin dosage (0.0 to 30.0)\n",
    "        \n",
    "        Returns:\n",
    "            observation, reward, done, info\n",
    "        \"\"\"\n",
    "        action = float(action)\n",
    "        baseline = 15.0\n",
    "        mean_noise = 0.0\n",
    "        std_noise = 5.0\n",
    "        noise = np.random.normal(mean_noise, std_noise)\n",
    "        factor = 0.5\n",
    "        delta = (action - baseline) * factor + noise\n",
    "        self.current_glucose = self.current_glucose + delta\n",
    "        self.current_glucose = np.clip(self.current_glucose, 40.0, 300.0)\n",
    "\n",
    "        self.time_step += 1\n",
    "        done = self.time_step >= self.max_steps\n",
    "\n",
    "        if self.current_glucose < 70.0:\n",
    "            reward = -1.0\n",
    "        elif self.current_glucose > 180.0:\n",
    "            reward = -0.5\n",
    "        else:\n",
    "            reward = 1.0\n",
    "\n",
    "        info = {'glucose': self.current_glucose}\n",
    "\n",
    "        return self.current_glucose, reward, done, info\n",
    "\n",
    "    def close(self) -> None:\n",
    "        \"\"\"Close environment and cleanup resources.\"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SIMGLUCOSE ENVIRONMENT WRAPPER\n",
    "# ============================================================================\n",
    "\n",
    "class SimglucoseGymEnv(gymnasium.Env):\n",
    "    \"\"\"\n",
    "    Gymnasium-compatible wrapper for SimGlucose T1D environment.\n",
    "    Uses MockT1DEnv as the underlying simulation.\n",
    "    \"\"\"\n",
    "\n",
    "    metadata = {\"render_modes\": [\"human\"], \"render_fps\": 30}\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        patient_name: str = 'adolescent#001',\n",
    "        seed: Optional[int] = None,\n",
    "        render_mode: Optional[str] = None,\n",
    "        verbose: bool = False\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the SimglucoseGymEnv.\n",
    "        \n",
    "        Args:\n",
    "            patient_name: Name of patient profile (default: adolescent#001)\n",
    "            seed: Random seed for reproducibility\n",
    "            render_mode: Render mode (not used, kept for compatibility)\n",
    "            verbose:  Whether to print initialization messages\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.render_mode = render_mode\n",
    "        self.patient_name = patient_name\n",
    "        self.verbose = verbose\n",
    "        self._episode_steps = 0\n",
    "        self._max_episode_steps = 480\n",
    "        self._episode_rewards = []\n",
    "        self._last_obs = None\n",
    "\n",
    "        # Initialize mock environment\n",
    "        self. env = MockT1DEnv()\n",
    "        if self.verbose:\n",
    "            print(f\"OK Successfully initialized MockT1DEnv\")\n",
    "\n",
    "        # Define action and observation spaces\n",
    "        self.action_space = gymnasium.spaces.Box(\n",
    "            low=np.float32(0.0),\n",
    "            high=np.float32(30.0),\n",
    "            shape=(1,),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "\n",
    "        self.observation_space = gymnasium.spaces.Box(\n",
    "            low=np.float32(0.0),\n",
    "            high=np.float32(1000.0),\n",
    "            shape=(1,),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "\n",
    "        # Set seed if provided\n",
    "        if seed is not None:\n",
    "            self.seed(seed)\n",
    "\n",
    "    def seed(self, seed: Optional[int] = None) -> List[int]:\n",
    "        \"\"\"Set random seed for reproducibility.\"\"\"\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "            return [seed]\n",
    "        return []\n",
    "\n",
    "    def step(self, action: np.ndarray) -> Tuple[np.ndarray, float, bool, bool, Dict]: \n",
    "        \"\"\"\n",
    "        Perform one step in the environment.\n",
    "        \n",
    "        Args:\n",
    "            action: Action array from policy\n",
    "        \n",
    "        Returns:\n",
    "            observation, reward, terminated, truncated, info\n",
    "        \"\"\"\n",
    "        # Convert action to scalar\n",
    "        if isinstance(action, np.ndarray):\n",
    "            scalar_action = float(action[0]) if action.size == 1 else float(action. flat[0])\n",
    "        else:\n",
    "            scalar_action = float(action)\n",
    "\n",
    "        # Clip action to valid range\n",
    "        scalar_action = np.clip(scalar_action, 0.0, 30.0)\n",
    "\n",
    "        try:\n",
    "            observation, reward, done, info = self.env.step(scalar_action)\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR in step: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            raise\n",
    "\n",
    "        self._episode_steps += 1\n",
    "        self._episode_rewards. append(float(reward))\n",
    "\n",
    "        # Convert observation to proper format\n",
    "        observation = self._format_observation(observation)\n",
    "        self._last_obs = observation.copy()\n",
    "\n",
    "        # Check if episode is truncated\n",
    "        truncated = self._episode_steps >= self._max_episode_steps\n",
    "\n",
    "        return observation, float(reward), bool(done), truncated, info\n",
    "\n",
    "    def reset(\n",
    "        self,\n",
    "        seed: Optional[int] = None,\n",
    "        options: Optional[Dict] = None\n",
    "    ) -> Tuple[np.ndarray, Dict]:\n",
    "        \"\"\"\n",
    "        Reset the environment.\n",
    "        \n",
    "        Args:\n",
    "            seed: Random seed\n",
    "            options: Additional reset options\n",
    "        \n",
    "        Returns:\n",
    "            observation, info\n",
    "        \"\"\"\n",
    "        super().reset(seed=seed)\n",
    "\n",
    "        # Set seed if provided\n",
    "        if seed is not None:\n",
    "            np.random. seed(seed)\n",
    "\n",
    "        try:\n",
    "            observation = self.env.reset()\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR in reset: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            raise\n",
    "\n",
    "        # Format observation\n",
    "        observation = self._format_observation(observation)\n",
    "        self._last_obs = observation.copy()\n",
    "        self._episode_steps = 0\n",
    "        self._episode_rewards = []\n",
    "\n",
    "        return observation, {}\n",
    "\n",
    "    def _format_observation(self, obs: Any) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Convert observation to proper numpy format.\n",
    "        \n",
    "        Args:\n",
    "            obs: Raw observation from environment\n",
    "        \n",
    "        Returns:\n",
    "            Formatted observation as numpy array with shape (1,)\n",
    "        \"\"\"\n",
    "        if obs is None:\n",
    "            obs = self._last_obs if self._last_obs is not None else np.array([0.0], dtype=np.float32)\n",
    "        else:\n",
    "            if not isinstance(obs, np.ndarray):\n",
    "                obs = np.array([float(obs)], dtype=np.float32)\n",
    "            else:\n",
    "                if obs.ndim == 0:\n",
    "                    obs = np.array([float(obs)], dtype=np.float32)\n",
    "                elif obs.shape == (1,):\n",
    "                    obs = obs.astype(np. float32)\n",
    "                else: \n",
    "                    obs = np.array([float(obs.flat[0])], dtype=np.float32)\n",
    "        \n",
    "        return obs\n",
    "\n",
    "    def render(self) -> Optional[Any]:\n",
    "        \"\"\"Render the environment (not implemented).\"\"\"\n",
    "        return None\n",
    "\n",
    "    def close(self) -> None:\n",
    "        \"\"\"Close the environment and cleanup resources.\"\"\"\n",
    "        if hasattr(self, 'env') and self.env is not None:\n",
    "            self.env.close()\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# UTILITY FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def _ensure_numpy_array(value: Any, dtype: type = np.float32) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Utility function to ensure value is a numpy array.\n",
    "    \n",
    "    Args:\n",
    "        value: Value to convert\n",
    "        dtype: Target data type\n",
    "    \n",
    "    Returns: \n",
    "        Numpy array\n",
    "    \"\"\"\n",
    "    if isinstance(value, np.ndarray):\n",
    "        return value. astype(dtype)\n",
    "    return np.array([value], dtype=dtype)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# ENVIRONMENT SETUP AND TESTING\n",
    "# ============================================================================\n",
    "\n",
    "def setup_simglucose_environment(\n",
    "    patient_name: str = 'adolescent#001',\n",
    "    seed:  int = 42,\n",
    "    verbose: bool = True\n",
    ") -> SimglucoseGymEnv:\n",
    "    \"\"\"\n",
    "    Setup and initialize a SimGlucose environment.\n",
    "    \n",
    "    Args:\n",
    "        patient_name:  Patient profile name\n",
    "        seed: Random seed\n",
    "        verbose: Print status messages\n",
    "    \n",
    "    Returns: \n",
    "        Initialized SimglucoseGymEnv\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"ENVIRONMENT INITIALIZATION\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"Patient:  {patient_name}\")\n",
    "        print(f\"Seed: {seed}\")\n",
    "    \n",
    "    env = SimglucoseGymEnv(\n",
    "        patient_name=patient_name,\n",
    "        seed=seed,\n",
    "        verbose=verbose\n",
    "    )\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"OK SimglucoseGymEnv initialized successfully!\")\n",
    "        print(\"=\" * 80 + \"\\n\")\n",
    "    \n",
    "    return env\n",
    "\n",
    "\n",
    "def test_environment(\n",
    "    env: SimglucoseGymEnv,\n",
    "    n_steps: int = 5,\n",
    "    seed: int = 42\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Test the environment with random actions.\n",
    "    \n",
    "    Args:\n",
    "        env: Environment to test\n",
    "        n_steps: Number of steps to test\n",
    "        seed: Random seed\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with test statistics\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"ENVIRONMENT TEST\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    obs, info = env.reset(seed=seed)\n",
    "    print(f\"✅ Reset successful!\")\n",
    "    print(f\"Initial Observation: {obs}\")\n",
    "    print(f\"Observation shape: {obs.shape}\")\n",
    "    print(f\"Observation dtype: {obs.dtype}\")\n",
    "    print(\"\\n--- Testing Steps ---\")\n",
    "    \n",
    "    episode_rewards = []\n",
    "    episode_glucose = []\n",
    "\n",
    "    for i in range(n_steps):\n",
    "        action = env. action_space.sample()\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        episode_rewards.append(reward)\n",
    "        \n",
    "        glucose = info.get('glucose', obs[0])\n",
    "        episode_glucose. append(glucose)\n",
    "\n",
    "        print(f\"Step {i + 1}:\")\n",
    "        print(f\"  Action: {action[0]:.2f} | Glucose: {glucose:.2f} | Reward: {reward:.4f}\")\n",
    "\n",
    "        if terminated or truncated: \n",
    "            print(\"  ⚠️  Episode ended!\")\n",
    "            break\n",
    "\n",
    "    # Summary statistics\n",
    "    print(f\"\\n--- Episode Summary ---\")\n",
    "    print(f\"  Total Steps: {len(episode_rewards)}\")\n",
    "    print(f\"  Total Return: {sum(episode_rewards):.4f}\")\n",
    "    print(f\"  Average Reward: {np.mean(episode_rewards):.4f}\")\n",
    "    print(f\"  Mean Glucose: {np.mean(episode_glucose):.2f} mg/dL\")\n",
    "    print(f\"  Glucose Range: [{np.min(episode_glucose):.2f}, {np.max(episode_glucose):.2f}]\")\n",
    "    print(\"=\" * 80 + \"\\n\")\n",
    "\n",
    "    return {\n",
    "        'episode_rewards': episode_rewards,\n",
    "        'episode_glucose': episode_glucose,\n",
    "        'total_return': sum(episode_rewards),\n",
    "        'mean_glucose': np.mean(episode_glucose)\n",
    "    }\n",
    "\n",
    "\n",
    "def define_behavior_policy(\n",
    "    observation: np.ndarray,\n",
    "    env: SimglucoseGymEnv,\n",
    "    policy_type: str = 'random'\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Behavior policy for data collection.\n",
    "    \n",
    "    Args:\n",
    "        observation: Current observation\n",
    "        env: Environment\n",
    "        policy_type: Type of policy ('random', 'mean', or 'follow_glucose')\n",
    "    \n",
    "    Returns:\n",
    "        Action array\n",
    "    \"\"\"\n",
    "    if policy_type == 'random': \n",
    "        return env.action_space. sample()\n",
    "    elif policy_type == 'mean':\n",
    "        # Return mean action\n",
    "        return np.array([(env.action_space.low[0] + env.action_space.high[0]) / 2.0], dtype=np.float32)\n",
    "    elif policy_type == 'follow_glucose':\n",
    "        # Simple reactive policy:  increase insulin if glucose is high\n",
    "        glucose = observation[0]\n",
    "        if glucose > 180:\n",
    "            action = 20.0\n",
    "        elif glucose > 140:\n",
    "            action = 15.0\n",
    "        else:\n",
    "            action = 10.0\n",
    "        return np.array([action], dtype=np.float32)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown policy type:  {policy_type}\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN EXECUTION (OPTIONAL)\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\": \n",
    "    # Setup environment\n",
    "    env = setup_simglucose_environment(\n",
    "        patient_name='adolescent#001',\n",
    "        seed=42,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    # Test environment\n",
    "    test_stats = test_environment(env, n_steps=5, seed=42)\n",
    "    \n",
    "    # Close environment\n",
    "    env.close()\n",
    "    print(\"✅ Environment test completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97778347-ee6d-4077-81a9-030821b306e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SIMGLUCOSE DATA COLLECTION PIPELINE\n",
      "================================================================================\n",
      "\n",
      "[1/5] Setting up environment...\n",
      "\n",
      "================================================================================\n",
      "ENVIRONMENT INITIALIZATION\n",
      "================================================================================\n",
      "Patient:  adolescent#001\n",
      "Seed: 42\n",
      "OK Successfully initialized MockT1DEnv\n",
      "OK SimglucoseGymEnv initialized successfully!\n",
      "================================================================================\n",
      "\n",
      "\n",
      "[2/5] Testing environment...\n",
      "\n",
      "================================================================================\n",
      "ENVIRONMENT TEST\n",
      "================================================================================\n",
      "✅ Reset successful!\n",
      "Initial Observation: [118.727005]\n",
      "Observation shape: (1,)\n",
      "Observation dtype: float32\n",
      "\n",
      "--- Testing Steps ---\n",
      "Step 1:\n",
      "  Action: 16.49 | Glucose: 113.91 | Reward: 1.0000\n",
      "Step 2:\n",
      "  Action: 27.35 | Glucose: 121.68 | Reward: 1.0000\n",
      "Step 3:\n",
      "  Action: 7.88 | Glucose: 119.52 | Reward: 1.0000\n",
      "Step 4:\n",
      "  Action: 2.16 | Glucose: 118.15 | Reward: 1.0000\n",
      "Step 5:\n",
      "  Action: 5.47 | Glucose: 110.48 | Reward: 1.0000\n",
      "\n",
      "--- Episode Summary ---\n",
      "  Total Steps: 5\n",
      "  Total Return: 5.0000\n",
      "  Average Reward: 1.0000\n",
      "  Mean Glucose: 116.75 mg/dL\n",
      "  Glucose Range: [110.48, 121.68]\n",
      "================================================================================\n",
      "\n",
      "\n",
      "[3/5] Setting up data collection...\n",
      "\n",
      "================================================================================\n",
      "Setting up Data Collection\n",
      "================================================================================\n",
      "Dataset Name: simglucose_adolescent_random_20260109_184503\n",
      "\n",
      "Data Collection Parameters:\n",
      "  - Number of Episodes: 10\n",
      "  - Max Steps per Episode: 480\n",
      "  - Record Infos: True\n",
      "\n",
      "✅ DataCollector created successfully!\n",
      "================================================================================\n",
      "\n",
      "[4/5] Using 'random' behavior policy...\n",
      "\n",
      "[5/5] Collecting data...\n",
      "\n",
      "================================================================================\n",
      "PHASE 1: DATA COLLECTION\n",
      "================================================================================\n",
      "Dataset:  simglucose_adolescent_random_20260109_184503\n",
      "Episodes: 10\n",
      "Max steps per episode: 480\n",
      "================================================================================\n",
      "\n",
      "Starting data collection for 10 episodes...\n",
      "  Collected 2/10 episodes...\n",
      "  Collected 4/10 episodes...\n",
      "  Collected 6/10 episodes...\n",
      "  Collected 8/10 episodes...\n",
      "  Collected 10/10 episodes...\n",
      "\n",
      "✅ OK Data collection complete! (took 0.07s)\n",
      "Collected 10 episodes\n",
      "   Total transitions: 4800\n",
      "   Average return: 172.7500 ± 127.1531\n",
      "\n",
      "================================================================================\n",
      "Collected Dataset Statistics\n",
      "================================================================================\n",
      "\n",
      "Overall Statistics:\n",
      "  Total Episodes: 10\n",
      "  Total Transitions:  4800\n",
      "  Mean Return: 172.7500 ± 127.1531\n",
      "  Return Range: [-30.0000, 393.0000]\n",
      "  Mean Episode Length: 480.00 ± 0.00\n",
      "  Mean Reward: 0.3599 ± 0.8139\n",
      "\n",
      "First 3 episodes:\n",
      "  Episode 1:\n",
      "    Return: 69.0000\n",
      "    Length: 480 steps\n",
      "    Actions: min=0.07, max=29.87\n",
      "    Rewards: min=-0.5000, max=1.0000\n",
      "  Episode 2:\n",
      "    Return: 393.0000\n",
      "    Length: 480 steps\n",
      "    Actions: min=0.07, max=29.87\n",
      "    Rewards: min=-0.5000, max=1.0000\n",
      "  Episode 3:\n",
      "    Return: 205.0000\n",
      "    Length: 480 steps\n",
      "    Actions: min=0.05, max=29.91\n",
      "    Rewards: min=-1.0000, max=1.0000\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "✅ Data Collection Pipeline Complete!\n",
      "================================================================================\n",
      "\n",
      "✅ Data collection successful!\n",
      "Dataset name: simglucose_adolescent_random_20260109_184503\n",
      "Episodes collected: 10\n",
      "Total transitions: 4800\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Data Collection Module for Offline RL Pipeline\n",
    "Collects trajectory data from SimGlucose environment without minari dependency\n",
    "\"\"\"\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from typing import Callable, Dict, List, Tuple, Optional, Any\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# DATA COLLECTION\n",
    "# ============================================================================\n",
    "\n",
    "class SimpleDataCollector:\n",
    "    \"\"\"Custom data collector - no minari dependency required.\"\"\"\n",
    "    \n",
    "    def __init__(self, env: SimglucoseGymEnv, record_infos: bool = True, verbose: bool = True):\n",
    "        \"\"\"\n",
    "        Initialize data collector.\n",
    "        \n",
    "        Args:\n",
    "            env: SimGlucose environment\n",
    "            record_infos: Whether to record step infos\n",
    "            verbose: Print status messages\n",
    "        \"\"\"\n",
    "        self. env = env\n",
    "        self.record_infos = record_infos\n",
    "        self.verbose = verbose\n",
    "        self.episodes = []\n",
    "    \n",
    "    def collect(\n",
    "        self,\n",
    "        policy: Callable,\n",
    "        num_episodes: int = 10,\n",
    "        max_steps:  int = 480,\n",
    "        seed: Optional[int] = None\n",
    "    ) -> List[Dict[str, np.ndarray]]:\n",
    "        \"\"\"\n",
    "        Collect episodes using a policy.\n",
    "        \n",
    "        Args:\n",
    "            policy: Function that takes (obs, env) and returns action\n",
    "            num_episodes: Number of episodes to collect\n",
    "            max_steps: Maximum steps per episode\n",
    "            seed: Random seed for reproducibility\n",
    "        \n",
    "        Returns:\n",
    "            List of episode dictionaries\n",
    "        \"\"\"\n",
    "        if self.verbose:\n",
    "            print(f\"\\nStarting data collection for {num_episodes} episodes...\")\n",
    "            start_time = time.time()\n",
    "\n",
    "        episodes_data = []\n",
    "\n",
    "        for episode_num in range(num_episodes):\n",
    "            # Reset environment\n",
    "            if seed is not None:\n",
    "                episode_seed = seed + episode_num\n",
    "            else:\n",
    "                episode_seed = None\n",
    "            \n",
    "            obs, info = self.env.reset(seed=episode_seed)\n",
    "\n",
    "            # Initialize episode data lists\n",
    "            observations = []\n",
    "            actions = []\n",
    "            rewards = []\n",
    "            terminations = []\n",
    "            truncations = []\n",
    "            infos = []\n",
    "            \n",
    "            episode_return = 0.0\n",
    "            episode_length = 0\n",
    "\n",
    "            # Collect steps\n",
    "            for step in range(max_steps):\n",
    "                # Store observation\n",
    "                observations.append(obs. copy())\n",
    "                \n",
    "                # Get action from policy\n",
    "                action = policy(obs, self.env)\n",
    "                actions.append(action)\n",
    "\n",
    "                # Take environment step\n",
    "                obs, reward, terminated, truncated, step_info = self.env.step(action)\n",
    "                \n",
    "                # Store transition data\n",
    "                rewards.append(reward)\n",
    "                terminations.append(terminated)\n",
    "                truncations.append(truncated)\n",
    "                \n",
    "                if self.record_infos:\n",
    "                    infos.append(step_info)\n",
    "                \n",
    "                episode_return += reward\n",
    "                episode_length += 1\n",
    "\n",
    "                # Check termination\n",
    "                if terminated or truncated: \n",
    "                    break\n",
    "\n",
    "            # Convert lists to numpy arrays and create episode dict\n",
    "            episode_dict = {\n",
    "                'observations': np. array(observations, dtype=np.float32),\n",
    "                'actions': np.array(actions, dtype=np.float32),\n",
    "                # Reshape rewards to 2D:  (n_steps, 1)\n",
    "                'rewards': np.array(rewards, dtype=np.float32).reshape(-1, 1),\n",
    "                'terminations': np.array(terminations, dtype=bool),\n",
    "                'truncations': np.array(truncations, dtype=bool),\n",
    "                'episode_return': float(episode_return),\n",
    "                'episode_length': int(episode_length)\n",
    "            }\n",
    "            \n",
    "            if self.record_infos:\n",
    "                episode_dict['infos'] = infos\n",
    "            \n",
    "            episodes_data.append(episode_dict)\n",
    "            \n",
    "            # Progress update\n",
    "            if self.verbose and (episode_num + 1) % max(1, num_episodes // 5) == 0:\n",
    "                print(f\"  Collected {episode_num + 1}/{num_episodes} episodes...\")\n",
    "\n",
    "        # Calculate statistics\n",
    "        elapsed_time = time.time() - start_time if self.verbose else 0\n",
    "        total_transitions = sum(ep['episode_length'] for ep in episodes_data)\n",
    "        avg_return = np.mean([ep['episode_return'] for ep in episodes_data])\n",
    "        std_return = np.std([ep['episode_return'] for ep in episodes_data])\n",
    "\n",
    "        if self.verbose:\n",
    "            print(f\"\\n✅ OK Data collection complete! (took {elapsed_time:.2f}s)\")\n",
    "            print(f\"Collected {len(episodes_data)} episodes\")\n",
    "            print(f\"   Total transitions: {total_transitions}\")\n",
    "            print(f\"   Average return: {avg_return:.4f} ± {std_return:.4f}\")\n",
    "\n",
    "        self.episodes = episodes_data\n",
    "        return episodes_data\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SETUP DATA COLLECTION\n",
    "# ============================================================================\n",
    "\n",
    "def setup_data_collection(\n",
    "    env: SimglucoseGymEnv,\n",
    "    num_episodes: int = 10,\n",
    "    max_steps_per_episode: int = 480,\n",
    "    dataset_name: Optional[str] = None,\n",
    "    record_infos: bool = True,\n",
    "    verbose: bool = True\n",
    ") -> Tuple[str, SimpleDataCollector]:\n",
    "    \"\"\"\n",
    "    Setup and prepare data collection. \n",
    "    \n",
    "    Args:\n",
    "        env: SimGlucose environment\n",
    "        num_episodes: Number of episodes to collect\n",
    "        max_steps_per_episode: Max steps per episode\n",
    "        dataset_name: Name for the dataset (auto-generated if None)\n",
    "        record_infos: Whether to record step infos\n",
    "        verbose: Print status messages\n",
    "    \n",
    "    Returns: \n",
    "        Tuple of (dataset_name, data_collector)\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"Setting up Data Collection\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "    # Generate dataset name if not provided\n",
    "    if dataset_name is None:\n",
    "        timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "        dataset_name = f'simglucose_adolescent_random_{timestamp}'\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Dataset Name: {dataset_name}\")\n",
    "        print(f\"\\nData Collection Parameters:\")\n",
    "        print(f\"  - Number of Episodes: {num_episodes}\")\n",
    "        print(f\"  - Max Steps per Episode: {max_steps_per_episode}\")\n",
    "        print(f\"  - Record Infos: {record_infos}\")\n",
    "\n",
    "    # Initialize data collector\n",
    "    data_collector = SimpleDataCollector(\n",
    "        env=env,\n",
    "        record_infos=record_infos,\n",
    "        verbose=verbose\n",
    "    )\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\n✅ DataCollector created successfully!\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "    return dataset_name, data_collector\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# COLLECT DATA\n",
    "# ============================================================================\n",
    "\n",
    "def collect_data_simple(\n",
    "    env: SimglucoseGymEnv,\n",
    "    policy: Callable,\n",
    "    num_episodes: int = 10,\n",
    "    max_steps_per_episode: int = 480,\n",
    "    dataset_name: str = 'simglucose-adolescent-random-v0',\n",
    "    seed: Optional[int] = None,\n",
    "    verbose: bool = True\n",
    ") -> List[Dict[str, np.ndarray]]:\n",
    "    \"\"\"\n",
    "    Collect trajectory data and return as list of episode dictionaries.\n",
    "    \n",
    "    Args:\n",
    "        env: SimGlucose environment\n",
    "        policy:  Behavior policy function\n",
    "        num_episodes: Number of episodes to collect\n",
    "        max_steps_per_episode: Max steps per episode\n",
    "        dataset_name: Name of dataset (for logging)\n",
    "        seed: Random seed for reproducibility\n",
    "        verbose: Print status messages\n",
    "    \n",
    "    Returns: \n",
    "        List of episode dictionaries with observations, actions, rewards, etc.\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"PHASE 1: DATA COLLECTION\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"Dataset:  {dataset_name}\")\n",
    "        print(f\"Episodes: {num_episodes}\")\n",
    "        print(f\"Max steps per episode: {max_steps_per_episode}\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "    # Create data collector\n",
    "    collector = SimpleDataCollector(env, record_infos=True, verbose=verbose)\n",
    "    \n",
    "    # Collect episodes\n",
    "    episodes_data = collector. collect(\n",
    "        policy=policy,\n",
    "        num_episodes=num_episodes,\n",
    "        max_steps=max_steps_per_episode,\n",
    "        seed=seed\n",
    "    )\n",
    "\n",
    "    return episodes_data\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# VISUALIZE COLLECTED DATA\n",
    "# ============================================================================\n",
    "\n",
    "def visualize_collected_data(\n",
    "    episodes_data: List[Dict[str, np.ndarray]],\n",
    "    num_episodes_to_show: int = 3,\n",
    "    verbose: bool = True\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Visualize and analyze collected episodes.\n",
    "    \n",
    "    Args:\n",
    "        episodes_data: List of collected episodes\n",
    "        num_episodes_to_show: Number of episodes to display details for\n",
    "        verbose: Print statistics\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with data statistics\n",
    "    \"\"\"\n",
    "    if verbose: \n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"Collected Dataset Statistics\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "    # Extract statistics\n",
    "    all_returns = [ep['episode_return'] for ep in episodes_data]\n",
    "    all_lengths = [ep['episode_length'] for ep in episodes_data]\n",
    "    all_rewards = np.concatenate([ep['rewards']. flatten() for ep in episodes_data])\n",
    "\n",
    "    stats = {\n",
    "        'num_episodes': len(episodes_data),\n",
    "        'total_transitions': int(sum(all_lengths)),\n",
    "        'mean_return': float(np.mean(all_returns)),\n",
    "        'std_return':  float(np.std(all_returns)),\n",
    "        'min_return': float(np.min(all_returns)),\n",
    "        'max_return': float(np.max(all_returns)),\n",
    "        'mean_length': float(np.mean(all_lengths)),\n",
    "        'std_length': float(np. std(all_lengths)),\n",
    "        'mean_reward': float(np.mean(all_rewards)),\n",
    "        'std_reward': float(np. std(all_rewards))\n",
    "    }\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"\\nOverall Statistics:\")\n",
    "        print(f\"  Total Episodes: {stats['num_episodes']}\")\n",
    "        print(f\"  Total Transitions:  {stats['total_transitions']}\")\n",
    "        print(f\"  Mean Return: {stats['mean_return']:.4f} ± {stats['std_return']:.4f}\")\n",
    "        print(f\"  Return Range: [{stats['min_return']:.4f}, {stats['max_return']:.4f}]\")\n",
    "        print(f\"  Mean Episode Length: {stats['mean_length']:.2f} ± {stats['std_length']:.2f}\")\n",
    "        print(f\"  Mean Reward: {stats['mean_reward']:.4f} ± {stats['std_reward']:.4f}\")\n",
    "\n",
    "        print(f\"\\nFirst {min(num_episodes_to_show, len(episodes_data))} episodes:\")\n",
    "        for i in range(min(num_episodes_to_show, len(episodes_data))):\n",
    "            ep = episodes_data[i]\n",
    "            print(f\"  Episode {i+1}:\")\n",
    "            print(f\"    Return: {ep['episode_return']:.4f}\")\n",
    "            print(f\"    Length: {ep['episode_length']} steps\")\n",
    "            print(f\"    Actions: min={ep['actions']. min():.2f}, max={ep['actions'].max():.2f}\")\n",
    "            print(f\"    Rewards: min={ep['rewards'].min():.4f}, max={ep['rewards'].max():.4f}\")\n",
    "        \n",
    "        print(\"=\" * 80 + \"\\n\")\n",
    "\n",
    "    return stats\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# COMPLETE DATA COLLECTION PIPELINE\n",
    "# ============================================================================\n",
    "\n",
    "def collect_data_and_save(\n",
    "    num_episodes: int = 10,\n",
    "    max_steps_per_episode: int = 480,\n",
    "    patient_name: str = 'adolescent#001',\n",
    "    policy_type: str = 'random',\n",
    "    seed: int = 42,\n",
    "    verbose: bool = True\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Complete SimGlucose environment setup and data collection pipeline.\n",
    "    \n",
    "    Args:\n",
    "        num_episodes: Number of episodes to collect\n",
    "        max_steps_per_episode:  Max steps per episode\n",
    "        patient_name: Patient profile name\n",
    "        policy_type: Type of behavior policy ('random', 'mean', 'follow_glucose')\n",
    "        seed: Random seed for reproducibility\n",
    "        verbose: Print status messages\n",
    "    \n",
    "    Returns: \n",
    "        Dictionary containing episodes, dataset name, and statistics\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"SIMGLUCOSE DATA COLLECTION PIPELINE\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "    # Setup environment\n",
    "    if verbose:\n",
    "        print(\"\\n[1/5] Setting up environment...\")\n",
    "    env = setup_simglucose_environment(\n",
    "        patient_name=patient_name,\n",
    "        seed=seed,\n",
    "        verbose=verbose\n",
    "    )\n",
    "\n",
    "    # Test environment\n",
    "    if verbose:\n",
    "        print(\"\\n[2/5] Testing environment...\")\n",
    "    test_stats = test_environment(env, n_steps=5, seed=seed)\n",
    "\n",
    "    # Setup data collection\n",
    "    if verbose: \n",
    "        print(\"\\n[3/5] Setting up data collection...\")\n",
    "    dataset_name, collector = setup_data_collection(\n",
    "        env=env,\n",
    "        num_episodes=num_episodes,\n",
    "        max_steps_per_episode=max_steps_per_episode,\n",
    "        verbose=verbose\n",
    "    )\n",
    "\n",
    "    # Define behavior policy\n",
    "    if verbose: \n",
    "        print(f\"\\n[4/5] Using '{policy_type}' behavior policy...\")\n",
    "    policy = lambda obs, env:  define_behavior_policy(obs, env, policy_type=policy_type)\n",
    "\n",
    "    # Collect data\n",
    "    if verbose: \n",
    "        print(f\"\\n[5/5] Collecting data...\")\n",
    "    episodes_data = collect_data_simple(\n",
    "        env=env,\n",
    "        policy=policy,\n",
    "        num_episodes=num_episodes,\n",
    "        max_steps_per_episode=max_steps_per_episode,\n",
    "        dataset_name=dataset_name,\n",
    "        seed=seed,\n",
    "        verbose=verbose\n",
    "    )\n",
    "\n",
    "    # Visualize data\n",
    "    data_stats = visualize_collected_data(episodes_data, num_episodes_to_show=3, verbose=verbose)\n",
    "\n",
    "    # Close environment\n",
    "    env.close()\n",
    "\n",
    "    if verbose:\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"✅ Data Collection Pipeline Complete!\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "    return {\n",
    "        'episodes_data': episodes_data,\n",
    "        'dataset_name': dataset_name,\n",
    "        'num_episodes': num_episodes,\n",
    "        'max_steps_per_episode': max_steps_per_episode,\n",
    "        'statistics': data_stats\n",
    "    }\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN EXECUTION\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Execute complete data collection pipeline\n",
    "    result = collect_data_and_save(\n",
    "        num_episodes=10,\n",
    "        max_steps_per_episode=480,\n",
    "        patient_name='adolescent#001',\n",
    "        policy_type='random',\n",
    "        seed=42,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    # Access results\n",
    "    episodes_data = result['episodes_data']\n",
    "    dataset_name = result['dataset_name']\n",
    "    statistics = result['statistics']\n",
    "    \n",
    "    print(f\"\\n✅ Data collection successful!\")\n",
    "    print(f\"Dataset name: {dataset_name}\")\n",
    "    print(f\"Episodes collected: {statistics['num_episodes']}\")\n",
    "    print(f\"Total transitions: {statistics['total_transitions']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c95a2d8f-292d-4aca-a526-909dd01221b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Dataset Management Module\n",
      "================================================================================\n",
      "\n",
      "To use this module, call main() with:\n",
      "  - episodes_data: List of collected episodes\n",
      "  - d3rlpy_module: Imported d3rlpy\n",
      "\n",
      "Example:\n",
      "  dataset, replay_buffer = main(episodes_data, d3rlpy)\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Dataset Management Module for Offline RL Pipeline\n",
    "Handles loading, creating, and managing datasets for training\n",
    "\"\"\"\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from typing import Dict, List, Any, Optional, Tuple\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# DATASET MANAGEMENT (WITHOUT MINARI DEPENDENCY)\n",
    "# ============================================================================\n",
    "\n",
    "class DatasetManager:\n",
    "    \"\"\"Handler for managing collected episode data as datasets.\"\"\"\n",
    "    \n",
    "    def __init__(self, verbose: bool = True):\n",
    "        \"\"\"\n",
    "        Initialize dataset manager.\n",
    "        \n",
    "        Args:\n",
    "            verbose: Print status messages\n",
    "        \"\"\"\n",
    "        self.verbose = verbose\n",
    "        self.dataset = None\n",
    "        self.statistics = None\n",
    "    \n",
    "    def create_dataset(\n",
    "        self,\n",
    "        episodes_data: List[Dict[str, np.ndarray]],\n",
    "        dataset_name: str = 'simglucose-offline-dataset',\n",
    "        description: str = ''\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Create a dataset from collected episodes.\n",
    "        \n",
    "        Args:\n",
    "            episodes_data: List of episode dictionaries from data collection\n",
    "            dataset_name:  Name for the dataset\n",
    "            description: Dataset description\n",
    "        \n",
    "        Returns:\n",
    "            Dataset dictionary with metadata and episodes\n",
    "        \"\"\"\n",
    "        if self. verbose:\n",
    "            print(f\"\\n{'='*80}\")\n",
    "            print(f\"Creating Dataset: {dataset_name}\")\n",
    "            print(f\"{'='*80}\")\n",
    "        \n",
    "        # Validate episodes\n",
    "        if not episodes_data:\n",
    "            raise ValueError(\"No episodes provided for dataset creation\")\n",
    "        \n",
    "        dataset = {\n",
    "            'name': dataset_name,\n",
    "            'description': description,\n",
    "            'creation_time': time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            'num_episodes': len(episodes_data),\n",
    "            'episodes': episodes_data,\n",
    "            'metadata': self._extract_metadata(episodes_data)\n",
    "        }\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f\"✅ Dataset created successfully!\")\n",
    "            print(f\"  - Name: {dataset_name}\")\n",
    "            print(f\"  - Episodes: {len(episodes_data)}\")\n",
    "            print(f\"  - Total transitions: {dataset['metadata']['total_transitions']}\")\n",
    "            print(f\"{'='*80}\\n\")\n",
    "        \n",
    "        self.dataset = dataset\n",
    "        return dataset\n",
    "    \n",
    "    @staticmethod\n",
    "    def _extract_metadata(episodes_data: List[Dict[str, np.ndarray]]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Extract metadata from episodes. \n",
    "        \n",
    "        Args:\n",
    "            episodes_data: List of episode dictionaries\n",
    "        \n",
    "        Returns: \n",
    "            Metadata dictionary\n",
    "        \"\"\"\n",
    "        episode_returns = [ep['episode_return'] for ep in episodes_data]\n",
    "        episode_lengths = [ep['episode_length'] for ep in episodes_data]\n",
    "        all_rewards = np.concatenate([ep['rewards']. flatten() for ep in episodes_data])\n",
    "        \n",
    "        return {\n",
    "            'total_transitions': int(sum(episode_lengths)),\n",
    "            'total_episodes': len(episodes_data),\n",
    "            'mean_episode_return': float(np.mean(episode_returns)),\n",
    "            'std_episode_return': float(np.std(episode_returns)),\n",
    "            'max_episode_return': float(np.max(episode_returns)),\n",
    "            'min_episode_return':  float(np.min(episode_returns)),\n",
    "            'mean_episode_length': float(np.mean(episode_lengths)),\n",
    "            'std_episode_length':  float(np.std(episode_lengths)),\n",
    "            'mean_reward': float(np.mean(all_rewards)),\n",
    "            'std_reward': float(np. std(all_rewards)),\n",
    "            'min_reward': float(np.min(all_rewards)),\n",
    "            'max_reward': float(np.max(all_rewards))\n",
    "        }\n",
    "    \n",
    "    def compute_statistics(self, episodes_data: Optional[List[Dict]] = None) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Compute comprehensive statistics about the dataset.\n",
    "        \n",
    "        Args:\n",
    "            episodes_data: Episodes to compute stats for (uses self.dataset if None)\n",
    "        \n",
    "        Returns:\n",
    "            Statistics dictionary\n",
    "        \"\"\"\n",
    "        if episodes_data is None:\n",
    "            if self.dataset is None:\n",
    "                raise ValueError(\"No dataset available.  Create or load a dataset first.\")\n",
    "            episodes_data = self.dataset['episodes']\n",
    "        \n",
    "        episode_returns = [ep['episode_return'] for ep in episodes_data]\n",
    "        episode_lengths = [ep['episode_length'] for ep in episodes_data]\n",
    "        all_rewards = np.concatenate([ep['rewards'].flatten() for ep in episodes_data])\n",
    "        \n",
    "        # Compute reward distribution statistics\n",
    "        reward_percentiles = {\n",
    "            'p25': float(np.percentile(all_rewards, 25)),\n",
    "            'p50': float(np. percentile(all_rewards, 50)),\n",
    "            'p75':  float(np.percentile(all_rewards, 75))\n",
    "        }\n",
    "        \n",
    "        stats = {\n",
    "            'num_episodes': len(episodes_data),\n",
    "            'total_transitions':  int(sum(episode_lengths)),\n",
    "            'mean_episode_return': float(np.mean(episode_returns)),\n",
    "            'std_episode_return':  float(np.std(episode_returns)),\n",
    "            'max_episode_return': float(np.max(episode_returns)),\n",
    "            'min_episode_return': float(np.min(episode_returns)),\n",
    "            'mean_episode_length': float(np.mean(episode_lengths)),\n",
    "            'std_episode_length': float(np. std(episode_lengths)),\n",
    "            'mean_reward': float(np.mean(all_rewards)),\n",
    "            'std_reward': float(np.std(all_rewards)),\n",
    "            'min_reward': float(np.min(all_rewards)),\n",
    "            'max_reward':  float(np.max(all_rewards)),\n",
    "            'reward_percentiles':  reward_percentiles,\n",
    "        }\n",
    "        \n",
    "        self.statistics = stats\n",
    "        return stats\n",
    "    \n",
    "    def print_statistics(self, stats: Optional[Dict[str, Any]] = None, verbose: bool = True) -> None:\n",
    "        \"\"\"\n",
    "        Print dataset statistics in a formatted manner.\n",
    "        \n",
    "        Args:\n",
    "            stats: Statistics dictionary (uses self.statistics if None)\n",
    "            verbose: Whether to print (for compatibility)\n",
    "        \"\"\"\n",
    "        if stats is None:\n",
    "            stats = self.statistics\n",
    "        \n",
    "        if stats is None:\n",
    "            print(\"No statistics available.  Compute statistics first.\")\n",
    "            return\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"Dataset Statistics\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"\\nEpisode Information:\")\n",
    "        print(f\"  - Total Episodes: {stats['num_episodes']}\")\n",
    "        print(f\"  - Total Transitions:  {stats['total_transitions']}\")\n",
    "        print(f\"  - Mean Episode Length: {stats['mean_episode_length']:.1f} ± {stats['std_episode_length']:.1f}\")\n",
    "        \n",
    "        print(f\"\\nEpisode Returns:\")\n",
    "        print(f\"  - Mean:  {stats['mean_episode_return']:.4f} ± {stats['std_episode_return']:.4f}\")\n",
    "        print(f\"  - Range: [{stats['min_episode_return']:.4f}, {stats['max_episode_return']:.4f}]\")\n",
    "        \n",
    "        print(f\"\\nReward Statistics:\")\n",
    "        print(f\"  - Mean:  {stats['mean_reward']:. 4f} ± {stats['std_reward']:.4f}\")\n",
    "        print(f\"  - Range: [{stats['min_reward']:.4f}, {stats['max_reward']:.4f}]\")\n",
    "        print(f\"  - Percentiles:\")\n",
    "        print(f\"    - 25th: {stats['reward_percentiles']['p25']:.4f}\")\n",
    "        print(f\"    - 50th (median): {stats['reward_percentiles']['p50']:.4f}\")\n",
    "        print(f\"    - 75th: {stats['reward_percentiles']['p75']:. 4f}\")\n",
    "        print(f\"{'='*80}\\n\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# REPLAY BUFFER CREATION\n",
    "# ============================================================================\n",
    "\n",
    "class ReplayBufferManager:\n",
    "    \"\"\"Handler for converting episodes to d3rlpy ReplayBuffer.\"\"\"\n",
    "    \n",
    "    def __init__(self, verbose: bool = True):\n",
    "        \"\"\"\n",
    "        Initialize replay buffer manager.\n",
    "        \n",
    "        Args:\n",
    "            verbose: Print status messages\n",
    "        \"\"\"\n",
    "        self.verbose = verbose\n",
    "        self.buffer = None\n",
    "    \n",
    "    def create_replay_buffer(\n",
    "        self,\n",
    "        episodes_data: List[Dict[str, np.ndarray]],\n",
    "        d3rlpy_module: Any,\n",
    "        buffer_limit: int = 1000000\n",
    "    ) -> Any:\n",
    "        \"\"\"\n",
    "        Convert episode data to d3rlpy ReplayBuffer. \n",
    "        \n",
    "        Args:\n",
    "            episodes_data: List of episode dictionaries from data collection\n",
    "            d3rlpy_module:  Imported d3rlpy module\n",
    "            buffer_limit: Maximum buffer size\n",
    "        \n",
    "        Returns:\n",
    "            d3rlpy ReplayBuffer\n",
    "        \"\"\"\n",
    "        if self.verbose:\n",
    "            print(f\"\\n{'='*80}\")\n",
    "            print(\"Creating d3rlpy ReplayBuffer from Episodes\")\n",
    "            print(f\"{'='*80}\")\n",
    "        \n",
    "        try:\n",
    "            from d3rlpy. dataset import Episode\n",
    "            \n",
    "            # Validate inputs\n",
    "            if d3rlpy_module is None:\n",
    "                raise ValueError(\"d3rlpy_module cannot be None\")\n",
    "            \n",
    "            if not episodes_data:\n",
    "                raise ValueError(\"episodes_data cannot be empty\")\n",
    "            \n",
    "            # Convert to d3rlpy Episode format\n",
    "            d3rlpy_episodes = []\n",
    "            total_transitions = 0\n",
    "            failed_episodes = 0\n",
    "            \n",
    "            for ep_idx, ep in enumerate(episodes_data):\n",
    "                try:\n",
    "                    # Validate episode data\n",
    "                    if 'observations' not in ep or 'actions' not in ep or 'rewards' not in ep: \n",
    "                        raise ValueError(f\"Episode {ep_idx} missing required keys\")\n",
    "                    \n",
    "                    # Create Episode with proper format\n",
    "                    d3rlpy_ep = Episode(\n",
    "                        observations=ep['observations']. astype(np.float32),\n",
    "                        actions=ep['actions']. astype(np.float32),\n",
    "                        rewards=ep['rewards'].astype(np. float32),\n",
    "                        terminated=bool(ep['terminations'][-1]) if len(ep['terminations']) > 0 else True,\n",
    "                        truncated=ep. get('truncations', np.zeros(len(ep['terminations']), dtype=bool))\n",
    "                    )\n",
    "                    d3rlpy_episodes.append(d3rlpy_ep)\n",
    "                    total_transitions += len(ep['rewards'])\n",
    "                \n",
    "                except Exception as e:\n",
    "                    if self.verbose:\n",
    "                        print(f\"⚠️  Warning: Failed to convert episode {ep_idx}: {str(e)}\")\n",
    "                    failed_episodes += 1\n",
    "                    continue\n",
    "            \n",
    "            if not d3rlpy_episodes: \n",
    "                raise ValueError(f\"No episodes could be converted to d3rlpy format.  Failed: {failed_episodes}\")\n",
    "            \n",
    "            if self.verbose and failed_episodes > 0:\n",
    "                print(f\"⚠️  {failed_episodes} episode(s) skipped due to errors\")\n",
    "            \n",
    "            # Create FIFO replay buffer\n",
    "            self.buffer = d3rlpy_module.dataset.create_fifo_replay_buffer(\n",
    "                episodes=d3rlpy_episodes,\n",
    "                limit=buffer_limit\n",
    "            )\n",
    "            \n",
    "            if self.verbose:\n",
    "                print(f\"\\n✅ ReplayBuffer created successfully!\")\n",
    "                print(f\"  - Buffer size:  {self.buffer.size()} transitions\")\n",
    "                print(f\"  - Episodes: {len(d3rlpy_episodes)}\")\n",
    "                print(f\"  - Buffer limit: {buffer_limit}\")\n",
    "                print(f\"{'='*80}\\n\")\n",
    "            \n",
    "            return self.buffer\n",
    "        \n",
    "        except ImportError as e: \n",
    "            print(f\"❌ Error:  Could not import d3rlpy. dataset:  {e}\")\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error creating replay buffer: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            raise\n",
    "    \n",
    "    def get_buffer_info(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Get information about the current replay buffer.\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with buffer information\n",
    "        \"\"\"\n",
    "        if self.buffer is None:\n",
    "            return {'status': 'No buffer created yet'}\n",
    "        \n",
    "        return {\n",
    "            'buffer_size': self.buffer.size(),\n",
    "            'status': 'Created and ready for training'\n",
    "        }\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# DATASET PIPELINE\n",
    "# ============================================================================\n",
    "\n",
    "class DatasetPipeline:\n",
    "    \"\"\"Complete pipeline for dataset creation and replay buffer preparation.\"\"\"\n",
    "    \n",
    "    def __init__(self, verbose: bool = True):\n",
    "        \"\"\"\n",
    "        Initialize pipeline. \n",
    "        \n",
    "        Args:\n",
    "            verbose: Print status messages\n",
    "        \"\"\"\n",
    "        self.verbose = verbose\n",
    "        self.dataset_manager = DatasetManager(verbose=verbose)\n",
    "        self.buffer_manager = ReplayBufferManager(verbose=verbose)\n",
    "        self.dataset = None\n",
    "        self.replay_buffer = None\n",
    "    \n",
    "    def setup_and_create_buffer(\n",
    "        self,\n",
    "        episodes_data: List[Dict[str, np.ndarray]],\n",
    "        d3rlpy_module: Any,\n",
    "        dataset_name: str = 'simglucose-offline-dataset',\n",
    "        compute_stats: bool = True\n",
    "    ) -> Tuple[Dict[str, Any], Any]:\n",
    "        \"\"\"\n",
    "        Complete pipeline:  create dataset and prepare replay buffer.\n",
    "        \n",
    "        Args:\n",
    "            episodes_data:  Collected episode data\n",
    "            d3rlpy_module: Imported d3rlpy module\n",
    "            dataset_name: Name for the dataset\n",
    "            compute_stats:  Whether to compute and print statistics\n",
    "        \n",
    "        Returns: \n",
    "            Tuple of (dataset, replay_buffer)\n",
    "        \"\"\"\n",
    "        if self.verbose:\n",
    "            print(f\"\\n{'='*80}\")\n",
    "            print(\"DATASET PIPELINE\")\n",
    "            print(f\"{'='*80}\")\n",
    "        \n",
    "        # Validate inputs\n",
    "        if d3rlpy_module is None: \n",
    "            raise ValueError(\"d3rlpy_module cannot be None.  Make sure d3rlpy is imported.\")\n",
    "        \n",
    "        if not episodes_data:\n",
    "            raise ValueError(\"episodes_data cannot be empty\")\n",
    "        \n",
    "        # Step 1: Create dataset\n",
    "        if self.verbose:\n",
    "            print(\"\\n[1/3] Creating dataset...\")\n",
    "        self.dataset = self.dataset_manager.create_dataset(\n",
    "            episodes_data,\n",
    "            dataset_name=dataset_name\n",
    "        )\n",
    "        \n",
    "        # Step 2: Compute statistics\n",
    "        if compute_stats:\n",
    "            if self.verbose:\n",
    "                print(\"[2/3] Computing statistics...\")\n",
    "            stats = self.dataset_manager.compute_statistics(episodes_data)\n",
    "            self.dataset_manager.print_statistics(stats)\n",
    "        else:\n",
    "            if self.verbose:\n",
    "                print(\"[2/3] Skipping statistics computation...\")\n",
    "        \n",
    "        # Step 3: Create replay buffer\n",
    "        if self.verbose:\n",
    "            print(\"[3/3] Creating replay buffer...\")\n",
    "        self.replay_buffer = self.buffer_manager.create_replay_buffer(\n",
    "            episodes_data,\n",
    "            d3rlpy_module\n",
    "        )\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f\"✅ Dataset pipeline complete!\")\n",
    "            print(f\"{'='*80}\\n\")\n",
    "        \n",
    "        return self.dataset, self.replay_buffer\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# UTILITY FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def save_dataset(\n",
    "    dataset: Dict[str, Any],\n",
    "    filepath: str = './offline_rl_dataset.pkl',\n",
    "    verbose: bool = True\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Save dataset to disk.\n",
    "    \n",
    "    Args:\n",
    "        dataset: Dataset dictionary\n",
    "        filepath: Path to save to\n",
    "        verbose: Print status\n",
    "    \n",
    "    Returns:\n",
    "        Path to saved file\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(f\"\\nSaving dataset to {filepath}...\")\n",
    "    \n",
    "    try:\n",
    "        # Create directory if it doesn't exist\n",
    "        os.makedirs(os.path. dirname(filepath) or '.', exist_ok=True)\n",
    "        \n",
    "        # Save dataset using pickle\n",
    "        with open(filepath, 'wb') as f:\n",
    "            pickle.dump(dataset, f)\n",
    "        \n",
    "        # Get file size\n",
    "        file_size = os.path.getsize(filepath) / (1024 * 1024)  # Convert to MB\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"✅ Dataset saved successfully!\")\n",
    "            print(f\"  - File:  {filepath}\")\n",
    "            print(f\"  - Size: {file_size:.2f} MB\")\n",
    "        \n",
    "        return filepath\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error saving dataset: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def load_dataset(\n",
    "    filepath: str = './offline_rl_dataset.pkl',\n",
    "    verbose: bool = True\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Load dataset from disk. \n",
    "    \n",
    "    Args:\n",
    "        filepath: Path to load from\n",
    "        verbose: Print status\n",
    "    \n",
    "    Returns:\n",
    "        Dataset dictionary\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(f\"\\nLoading dataset from {filepath}...\")\n",
    "    \n",
    "    try:\n",
    "        if not os.path.exists(filepath):\n",
    "            raise FileNotFoundError(f\"Dataset file not found: {filepath}\")\n",
    "        \n",
    "        with open(filepath, 'rb') as f:\n",
    "            dataset = pickle.load(f)\n",
    "        \n",
    "        if verbose: \n",
    "            print(f\"✅ Dataset loaded successfully!\")\n",
    "            print(f\"  - Name: {dataset.get('name', 'Unknown')}\")\n",
    "            print(f\"  - Episodes: {dataset.get('num_episodes', 'Unknown')}\")\n",
    "            print(f\"  - Creation time: {dataset.get('creation_time', 'Unknown')}\")\n",
    "        \n",
    "        return dataset\n",
    "    \n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"❌ Dataset file not found: {filepath}\")\n",
    "        raise\n",
    "    except Exception as e: \n",
    "        print(f\"❌ Error loading dataset: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN EXECUTION\n",
    "# ============================================================================\n",
    "\n",
    "def main(\n",
    "    episodes_data: List[Dict[str, np.ndarray]],\n",
    "    d3rlpy_module: Any,\n",
    "    dataset_name: str = 'simglucose-offline-v1',\n",
    "    save_path: str = './simglucose_offline_dataset.pkl'\n",
    ") -> Tuple[Dict[str, Any], Any]:\n",
    "    \"\"\"\n",
    "    Complete dataset management pipeline.\n",
    "    \n",
    "    Args:\n",
    "        episodes_data:  Collected episode data\n",
    "        d3rlpy_module: Imported d3rlpy module\n",
    "        dataset_name: Name for the dataset\n",
    "        save_path: Path to save dataset\n",
    "    \n",
    "    Returns: \n",
    "        Tuple of (dataset, replay_buffer)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create pipeline\n",
    "        pipeline = DatasetPipeline(verbose=True)\n",
    "        \n",
    "        # Run pipeline\n",
    "        dataset, replay_buffer = pipeline.setup_and_create_buffer(\n",
    "            episodes_data=episodes_data,\n",
    "            d3rlpy_module=d3rlpy_module,\n",
    "            dataset_name=dataset_name,\n",
    "            compute_stats=True\n",
    "        )\n",
    "        \n",
    "        # Save dataset\n",
    "        save_dataset(dataset, filepath=save_path, verbose=True)\n",
    "        \n",
    "        return dataset, replay_buffer\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Pipeline execution failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        raise\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\": \n",
    "    \"\"\"\n",
    "    Example usage - uncomment and modify as needed:\n",
    "    \n",
    "    # Make sure these are defined from previous cells: \n",
    "    # - episodes_data (from data collection)\n",
    "    # - d3rlpy (imported module)\n",
    "    \n",
    "    dataset, replay_buffer = main(\n",
    "        episodes_data=episodes_data,\n",
    "        d3rlpy_module=d3rlpy,\n",
    "        dataset_name='simglucose-offline-v1',\n",
    "        save_path='./simglucose_offline_dataset. pkl'\n",
    "    )\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Dataset Management Module\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\\nTo use this module, call main() with:\")\n",
    "    print(\"  - episodes_data: List of collected episodes\")\n",
    "    print(\"  - d3rlpy_module: Imported d3rlpy\")\n",
    "    print(\"\\nExample:\")\n",
    "    print(\"  dataset, replay_buffer = main(episodes_data, d3rlpy)\")\n",
    "    print(\"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f2ee78c-4260-4e47-aea9-29d6f879111e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# OFFLINE RL ALGORITHM CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "class OfflineRLAlgorithm:\n",
    "    \"\"\"Factory for creating offline RL algorithms.\"\"\"\n",
    "\n",
    "    ALGORITHM_CONFIGS = {\n",
    "        'cql': {\n",
    "            'name': 'Conservative Q-Learning (CQL)',\n",
    "            'description': 'Best for general offline RL',\n",
    "            'config_class': d3rlpy.algos.CQLConfig,\n",
    "            'params': {\n",
    "                'actor_learning_rate': 1e-4,\n",
    "                'critic_learning_rate': 3e-4,\n",
    "                'batch_size': 256,\n",
    "                'gamma': 0.99,\n",
    "                'tau': 5e-3,\n",
    "                'alpha_learning_rate': 1e-4,\n",
    "                'conservative_weight': 10.0,\n",
    "                'n_action_samples': 10,\n",
    "            }\n",
    "        },\n",
    "        'iql': {\n",
    "            'name':  'Implicit Q-Learning (IQL)',\n",
    "            'description': 'Avoids querying unseen actions',\n",
    "            'config_class':  d3rlpy.algos.IQLConfig,\n",
    "            'params': {\n",
    "                'actor_learning_rate': 3e-4,\n",
    "                'critic_learning_rate': 3e-4,\n",
    "                'batch_size': 256,\n",
    "                'gamma': 0.99,\n",
    "                'tau': 5e-3,\n",
    "                'expectile':  0.7,\n",
    "                'weight_temp': 3.0,\n",
    "                'max_weight': 100.0,\n",
    "            }\n",
    "        },\n",
    "        'bc': {\n",
    "            'name': 'Behavioral Cloning (BC)',\n",
    "            'description': 'Simple imitation learning baseline',\n",
    "            'config_class':  d3rlpy.algos.BCConfig,\n",
    "            'params': {\n",
    "                'learning_rate': 1e-4,\n",
    "                'batch_size': 256,\n",
    "            }\n",
    "        },\n",
    "    }\n",
    "\n",
    "    @classmethod\n",
    "    def list_algorithms(cls) -> None:\n",
    "        \"\"\"List available algorithms.\"\"\"\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"Available Offline RL Algorithms\")\n",
    "        print(f\"{'='*80}\")\n",
    "        for algo_type, config in cls.ALGORITHM_CONFIGS.items():\n",
    "            print(f\"\\n  {algo_type.upper()}:\\n    Name: {config['name']}\\n    Description: {config['description']}\")\n",
    "\n",
    "    @classmethod\n",
    "    def create(cls, algo_type: str = 'cql', device: str = 'cpu:0', **custom_params):\n",
    "        \"\"\"Create an offline RL algorithm.\"\"\"\n",
    "        algo_type = algo_type.lower()\n",
    "\n",
    "        if algo_type not in cls.ALGORITHM_CONFIGS:\n",
    "            raise ValueError(f\"Unknown algorithm:  {algo_type}\")\n",
    "\n",
    "        config_info = cls.ALGORITHM_CONFIGS[algo_type]\n",
    "        params = config_info['params'].copy()\n",
    "        params.update(custom_params)\n",
    "\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Creating {config_info['name']}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"Device: {device}\")\n",
    "        print(f\"\\nHyperparameters:\")\n",
    "        for key, value in params.items():\n",
    "            print(f\"  - {key}: {value}\")\n",
    "\n",
    "        config = config_info['config_class'](**params)\n",
    "        algo = config.create(device=device)\n",
    "\n",
    "        print(f\"OK {config_info['name']} created successfully!\")\n",
    "        return algo\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# TRAINING\n",
    "# ============================================================================\n",
    "\n",
    "class OfflineRLTrainer:\n",
    "    \"\"\"Trainer for offline RL algorithms.\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def train(\n",
    "        algo,\n",
    "        replay_buffer,\n",
    "        n_steps:  int = 50000,\n",
    "        save_interval: int = 5000,\n",
    "        verbose: bool = True\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"Train offline RL algorithm.\"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"Starting Offline RL Training\")\n",
    "        print(\"=\"*80)\n",
    "        print(\"Training Configuration:\")\n",
    "        print(f\"  - Algorithm: {algo.__class__.__name__}\")\n",
    "        print(f\"  - Total Steps: {n_steps}\")\n",
    "        # ✅ Use . size() instead of len()\n",
    "        print(f\"  - Replay Buffer Size: {replay_buffer.size()}\")\n",
    "        print()\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        try:\n",
    "            # Train the algorithm\n",
    "            algo.fit(\n",
    "                replay_buffer,\n",
    "                n_steps=n_steps,\n",
    "                show_progress=verbose\n",
    "            )\n",
    "\n",
    "            training_time = time.time() - start_time\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"\\nOK Training complete!\")\n",
    "                print(f\"  - Training Time: {training_time:.2f}s\") # Fixed f-string format\n",
    "                print(f\"  - Steps per Second: {n_steps / training_time:.2f}\")\n",
    "\n",
    "            return {\n",
    "                'n_steps': n_steps,\n",
    "                'training_time': training_time,\n",
    "                'steps_per_second': n_steps / training_time\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            if verbose:\n",
    "                print(f\"ERROR during training: {e}\")\n",
    "            raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "432450f1-aa8e-4d2c-b4f5-7983fa639323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# EVALUATION\n",
    "# ============================================================================\n",
    "\n",
    "class PolicyEvaluator:\n",
    "    \"\"\"Handler for evaluating trained policies.\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def evaluate(\n",
    "        algo,\n",
    "        env,\n",
    "        n_episodes: int = 5,\n",
    "        max_steps:  Optional[int] = None,\n",
    "        verbose: bool = True\n",
    "    ) -> Dict[str, float]:\n",
    "        \"\"\"Evaluate a trained policy.\"\"\"\n",
    "        if verbose:\n",
    "            print(f\"\\n{'='*80}\")\n",
    "            print(f\"Evaluating Policy ({n_episodes} episodes)\")\n",
    "            print(f\"{'='*80}\")\n",
    "\n",
    "        episode_returns = []\n",
    "        episode_lengths = []\n",
    "\n",
    "        for episode_num in range(n_episodes):\n",
    "            obs, _ = env.reset()\n",
    "            total_return = 0.0\n",
    "            steps = 0\n",
    "\n",
    "            done = False\n",
    "            while not done:\n",
    "                action = algo.predict(np.expand_dims(obs, axis=0))[0]\n",
    "                obs, reward, terminated, truncated, _ = env.step(action)\n",
    "                total_return += reward\n",
    "                steps += 1\n",
    "\n",
    "                done = terminated or truncated\n",
    "                if max_steps and steps >= max_steps:\n",
    "                    done = True\n",
    "\n",
    "            episode_returns.append(total_return)\n",
    "            episode_lengths.append(steps)\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"  Episode {episode_num + 1:3d}: Return = {total_return:8.2f}, Steps = {steps:3d}\")\n",
    "\n",
    "        mean_return = float(np.mean(episode_returns))\n",
    "        std_return = float(np.std(episode_returns))\n",
    "        mean_length = float(np.mean(episode_lengths))\n",
    "\n",
    "        stats = {\n",
    "            'mean_return': mean_return,\n",
    "            'std_return': std_return,\n",
    "            'max_return': float(np.max(episode_returns)),\n",
    "            'min_return': float(np.min(episode_returns)),\n",
    "            'mean_episode_length': mean_length,\n",
    "        }\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"\\nOK Evaluation Complete!\")\n",
    "            print(f\"  - Mean Return: {mean_return:.4f} +/- {std_return:.4f}\")\n",
    "\n",
    "        return stats\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# MODEL PERSISTENCE\n",
    "# ============================================================================\n",
    "\n",
    "class ModelManager:\n",
    "    \"\"\"Handler for saving and loading models.\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def save_model(algo, save_path: str = \"./offline_rl_model\") -> None:\n",
    "        \"\"\"Save a trained model.\"\"\"\n",
    "        print(f\"\\nSaving model to {save_path}...\")\n",
    "        algo.save_model(save_path)\n",
    "        print(f\"OK Model saved successfully!\")\n",
    "\n",
    "    @staticmethod\n",
    "    def load_model(\n",
    "        algo_type: str,\n",
    "        save_path: str = \"./offline_rl_model\",\n",
    "        device: str = \"cpu:0\"\n",
    "    ):\n",
    "        \"\"\"Load a trained model.\"\"\"\n",
    "        print(f\"\\nLoading model from {save_path}...\")\n",
    "        config_class = OfflineRLAlgorithm.ALGORITHM_CONFIGS[algo_type.lower()]['config_class']\n",
    "        algo = config_class().create(device=device)\n",
    "        algo.load_model(save_path)\n",
    "        print(f\"OK Model loaded successfully!\")\n",
    "        return algo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3f16937-9d8f-420c-807e-883a2f9881b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# COMPLETE PIPELINE\n",
    "# ============================================================================\n",
    "\n",
    "def main(\n",
    "    episodes_data:      List[Dict],  # Accept episodes data directly\n",
    "    algo_type:  str = 'cql',\n",
    "    n_steps:  int = 100000,\n",
    "    n_eval_episodes: int = 5,\n",
    "    device:  str = 'cpu:  0',\n",
    "    save_model_flag:  bool = True\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"Complete offline RL pipeline.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"OFFLINE DEEP REINFORCEMENT LEARNING PIPELINE\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    OfflineRLAlgorithm.  list_algorithms()\n",
    "\n",
    "    # Create replay buffer directly from episodes\n",
    "    try:\n",
    "        # ✅ Convert dict episodes to d3rlpy Episode objects\n",
    "        from d3rlpy.dataset import Episode\n",
    "\n",
    "        d3rlpy_episodes = []\n",
    "        for episode_dict in episodes_data:\n",
    "            # ✅ Episode requires (observations, actions, rewards, terminated:   bool)\n",
    "            # terminated should be True if episode ended, False otherwise\n",
    "            episode = Episode(\n",
    "                observations=episode_dict['observations'],\n",
    "                actions=episode_dict['actions'],\n",
    "                rewards=episode_dict['rewards'],\n",
    "                terminated=True  # ✅ Mark as terminated (episode finished)\n",
    "            )\n",
    "            d3rlpy_episodes.append(episode)\n",
    "\n",
    "        # Now create replay buffer with proper Episode objects\n",
    "        replay_buffer = d3rlpy.dataset.create_fifo_replay_buffer(\n",
    "            episodes=d3rlpy_episodes,\n",
    "            limit=1000000  # 1 million transitions max\n",
    "        )\n",
    "        print(f\"\\nOK ReplayBuffer created successfully!\")\n",
    "        # ✅ Use size() method or get total_transitions\n",
    "        buffer_size = replay_buffer.size()\n",
    "        print(f\"  - Size: {buffer_size} transitions\")\n",
    "        results['replay_buffer_size'] = buffer_size\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR creating replay buffer: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        raise\n",
    "\n",
    "    algo = OfflineRLAlgorithm.  create(algo_type=algo_type, device=device)\n",
    "    results['algorithm'] = algo_type\n",
    "\n",
    "    training_stats = OfflineRLTrainer. train(\n",
    "        algo=algo,\n",
    "        replay_buffer=replay_buffer,\n",
    "        n_steps=n_steps,\n",
    "        save_interval=max(1000, n_steps // 10),\n",
    "        verbose=True\n",
    "    )\n",
    "    results['training_stats'] = training_stats\n",
    "\n",
    "    try:\n",
    "        from gymnasium.  wrappers import TimeLimit\n",
    "        eval_env = TimeLimit(\n",
    "            SimglucoseGymEnv(patient_name='adolescent#001', seed=42),\n",
    "            max_episode_steps=480\n",
    "        )\n",
    "\n",
    "        eval_stats = PolicyEvaluator.evaluate(\n",
    "            algo=algo,\n",
    "            env=eval_env,\n",
    "            n_episodes=n_eval_episodes,\n",
    "            max_steps=480,\n",
    "            verbose=True\n",
    "        )\n",
    "        results['evaluation_stats'] = eval_stats\n",
    "        eval_env.close()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nWARNING Error during evaluation: {e}\")\n",
    "\n",
    "    if save_model_flag:\n",
    "        model_path = f\"./offline_rl_{algo_type}_model\"\n",
    "        ModelManager.save_model(algo, save_path=model_path)\n",
    "        results['model_path'] = model_path\n",
    "\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"TRAINING SUMMARY\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"OK Pipeline Complete!\")\n",
    "    print(f\"\\nResults:\")\n",
    "    print(f\"  - Algorithm: {results['algorithm']}\")\n",
    "    print(f\"  - Training Steps: {results['training_stats']['n_steps']}\")\n",
    "    print(f\"  - Training Time: {results['training_stats']['training_time']:.2f}s\")\n",
    "    print(f\"  - Replay Buffer Size: {results['replay_buffer_size']}\")\n",
    "\n",
    "    if 'evaluation_stats' in results:\n",
    "        eval_stats = results['evaluation_stats']\n",
    "        print(f\"  - Evaluation Mean Return: {eval_stats['mean_return']:.4f} +/- {eval_stats['std_return']:.4f}\")\n",
    "\n",
    "    if 'model_path' in results:\n",
    "        print(f\"  - Model Saved:   {results['model_path']}\")\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "610b6161-1697-468f-8434-ccb133b3d41f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PHASE 1: ENVIRONMENT VERIFICATION\n",
      "======================================================================\n",
      "\n",
      "================================================================================\n",
      "PHASE 2: DATA COLLECTION\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "SIMGLUCOSE DATA COLLECTION PIPELINE\n",
      "================================================================================\n",
      "\n",
      "[1/5] Setting up environment...\n",
      "\n",
      "================================================================================\n",
      "ENVIRONMENT INITIALIZATION\n",
      "================================================================================\n",
      "Patient:  adolescent#001\n",
      "Seed: 42\n",
      "OK Successfully initialized MockT1DEnv\n",
      "OK SimglucoseGymEnv initialized successfully!\n",
      "================================================================================\n",
      "\n",
      "\n",
      "[2/5] Testing environment...\n",
      "\n",
      "================================================================================\n",
      "ENVIRONMENT TEST\n",
      "================================================================================\n",
      "✅ Reset successful!\n",
      "Initial Observation: [118.727005]\n",
      "Observation shape: (1,)\n",
      "Observation dtype: float32\n",
      "\n",
      "--- Testing Steps ---\n",
      "Step 1:\n",
      "  Action: 25.80 | Glucose: 118.57 | Reward: 1.0000\n",
      "Step 2:\n",
      "  Action: 2.84 | Glucose: 114.09 | Reward: 1.0000\n",
      "Step 3:\n",
      "  Action: 28.95 | Glucose: 122.46 | Reward: 1.0000\n",
      "Step 4:\n",
      "  Action: 5.00 | Glucose: 122.51 | Reward: 1.0000\n",
      "Step 5:\n",
      "  Action: 14.87 | Glucose: 119.54 | Reward: 1.0000\n",
      "\n",
      "--- Episode Summary ---\n",
      "  Total Steps: 5\n",
      "  Total Return: 5.0000\n",
      "  Average Reward: 1.0000\n",
      "  Mean Glucose: 119.43 mg/dL\n",
      "  Glucose Range: [114.09, 122.51]\n",
      "================================================================================\n",
      "\n",
      "\n",
      "[3/5] Setting up data collection...\n",
      "\n",
      "================================================================================\n",
      "Setting up Data Collection\n",
      "================================================================================\n",
      "Dataset Name: simglucose_adolescent_random_20260109_184503\n",
      "\n",
      "Data Collection Parameters:\n",
      "  - Number of Episodes: 10\n",
      "  - Max Steps per Episode: 480\n",
      "  - Record Infos: True\n",
      "\n",
      "✅ DataCollector created successfully!\n",
      "================================================================================\n",
      "\n",
      "[4/5] Using 'random' behavior policy...\n",
      "\n",
      "[5/5] Collecting data...\n",
      "\n",
      "================================================================================\n",
      "PHASE 1: DATA COLLECTION\n",
      "================================================================================\n",
      "Dataset:  simglucose_adolescent_random_20260109_184503\n",
      "Episodes: 10\n",
      "Max steps per episode: 480\n",
      "================================================================================\n",
      "\n",
      "Starting data collection for 10 episodes...\n",
      "  Collected 2/10 episodes...\n",
      "  Collected 4/10 episodes...\n",
      "  Collected 6/10 episodes...\n",
      "  Collected 8/10 episodes...\n",
      "  Collected 10/10 episodes...\n",
      "\n",
      "✅ OK Data collection complete! (took 0.07s)\n",
      "Collected 10 episodes\n",
      "   Total transitions: 4800\n",
      "   Average return: 110.1000 ± 142.9468\n",
      "\n",
      "================================================================================\n",
      "Collected Dataset Statistics\n",
      "================================================================================\n",
      "\n",
      "Overall Statistics:\n",
      "  Total Episodes: 10\n",
      "  Total Transitions:  4800\n",
      "  Mean Return: 110.1000 ± 142.9468\n",
      "  Return Range: [-75.0000, 308.0000]\n",
      "  Mean Episode Length: 480.00 ± 0.00\n",
      "  Mean Reward: 0.2294 ± 0.8640\n",
      "\n",
      "First 3 episodes:\n",
      "  Episode 1:\n",
      "    Return: 7.0000\n",
      "    Length: 480 steps\n",
      "    Actions: min=0.02, max=30.00\n",
      "    Rewards: min=-1.0000, max=1.0000\n",
      "  Episode 2:\n",
      "    Return: 308.0000\n",
      "    Length: 480 steps\n",
      "    Actions: min=0.01, max=29.95\n",
      "    Rewards: min=-1.0000, max=1.0000\n",
      "  Episode 3:\n",
      "    Return: -75.0000\n",
      "    Length: 480 steps\n",
      "    Actions: min=0.16, max=29.97\n",
      "    Rewards: min=-1.0000, max=1.0000\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "✅ Data Collection Pipeline Complete!\n",
      "================================================================================\n",
      "OK Collected 5 episodes\n",
      "\n",
      "================================================================================\n",
      "PHASE 3: OFFLINE RL TRAINING\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "OFFLINE DEEP REINFORCEMENT LEARNING PIPELINE\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "Available Offline RL Algorithms\n",
      "================================================================================\n",
      "\n",
      "  CQL:\n",
      "    Name: Conservative Q-Learning (CQL)\n",
      "    Description: Best for general offline RL\n",
      "\n",
      "  IQL:\n",
      "    Name: Implicit Q-Learning (IQL)\n",
      "    Description: Avoids querying unseen actions\n",
      "\n",
      "  BC:\n",
      "    Name: Behavioral Cloning (BC)\n",
      "    Description: Simple imitation learning baseline\n",
      "ERROR creating replay buffer: string indices must be integers, not 'str'\n",
      "ERROR during training: string indices must be integers, not 'str'\n",
      "\n",
      "================================================================================\n",
      "OK COMPLETE OFFLINE DRL PIPELINE FINISHED!\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/n4/5y0dr95d4qn5qvlcvfwg85140000gn/T/ipykernel_66063/36382913.py\", line 32, in main\n",
      "    observations=episode_dict['observations'],\n",
      "                 ~~~~~~~~~~~~^^^^^^^^^^^^^^^^\n",
      "TypeError: string indices must be integers, not 'str'\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/n4/5y0dr95d4qn5qvlcvfwg85140000gn/T/ipykernel_66063/2549507451.py\", line 35, in <module>\n",
      "    results = main(\n",
      "        episodes_data=episodes_data,\n",
      "    ...<4 lines>...\n",
      "        save_model_flag=True\n",
      "    )\n",
      "  File \"/var/folders/n4/5y0dr95d4qn5qvlcvfwg85140000gn/T/ipykernel_66063/36382913.py\", line 32, in main\n",
      "    observations=episode_dict['observations'],\n",
      "                 ~~~~~~~~~~~~^^^^^^^^^^^^^^^^\n",
      "TypeError: string indices must be integers, not 'str'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# MAIN EXECUTION\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=\" * 70)\n",
    "    print(\"PHASE 1: ENVIRONMENT VERIFICATION\")\n",
    "    print(\"=\" * 70)\n",
    "    # ...  Phase 1 code unchanged ...\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"PHASE 2: DATA COLLECTION\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    episodes_data = None\n",
    "    try:\n",
    "        episodes_data = collect_data_and_save(\n",
    "            num_episodes=10,\n",
    "            max_steps_per_episode=480,\n",
    "            patient_name='adolescent#001',\n",
    "            policy_type='random'\n",
    "        )\n",
    "        print(f\"OK Collected {len(episodes_data)} episodes\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR during data collection: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"PHASE 3: OFFLINE RL TRAINING\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    try:\n",
    "        if episodes_data is not None:\n",
    "            results = main(\n",
    "                episodes_data=episodes_data,\n",
    "                algo_type='cql',\n",
    "                n_steps=50000,\n",
    "                n_eval_episodes=3,\n",
    "                device='cpu:0',\n",
    "                save_model_flag=True\n",
    "            )\n",
    "            print(\"\\n\" + \"=\" * 80)\n",
    "            print(\"OK TRAINING COMPLETE!\")\n",
    "            print(\"=\" * 80)\n",
    "        else:\n",
    "            print(\"WARNING: No episodes collected.  Skipping training.\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR during training: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"OK COMPLETE OFFLINE DRL PIPELINE FINISHED!\")\n",
    "    print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
